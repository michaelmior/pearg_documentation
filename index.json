[
{
	"uri": "/pearg_documentation/metadata-documentation/metadata-overview/",
	"title": "Metadata Overview",
	"tags": [],
	"description": "A overview of our metadata system in use.",
	"content": "This document describes our system of storing metadata for sequencing samples.\n"
},
{
	"uri": "/pearg_documentation/usage-documentation/running-jobs-with-slurm/",
	"title": "Running Jobs with SLURM",
	"tags": [],
	"description": "Documentation on how to run SLURM jobs.",
	"content": "Documentation on how to run SLURM jobs.\n"
},
{
	"uri": "/pearg_documentation/best-practices/directory-structure/",
	"title": "Directory Structure",
	"tags": [],
	"description": "How to structure your project directory when performing an analysis.",
	"content": " When using the PEARG clusters or the Melbourne Bioinformatics clusters, your files and data should be organised according to the convention described here.\nEach project should have its own directory located inside your home directory or a shared location. Your project directory name should be meaningful.\nAny shared data such as reference genomes and indices should be stored outside your project directory to avoid unnecessary duplication of data.\nCheck your project directory At absolute minimum, please check:\n is your project name sensible? do you have a README file (or equivalent) in the parent directory of your project directory that contains meaningful information? do you have a data directory (containing your raw data) and a results directory (containing processed data) with files organised in a logical manner?  If you\u0026rsquo;ve answered no to any of these questions, you may get a stern email in the future.\n  The motivation behind having a strict structure that all members adhere to for project organisation is transparency. Anyone from the lab should be able to look in your project directory and clearly understand what was run and reproduce your analysis. Most of the time, the person trying to decode what analyses were performed and why will be future you. Cooperate with your future self by leaving verbose notes in README files!\nWhile the sysadmin won\u0026rsquo;t be authoritarian about the precise directory structure, any flagrant disregard for the guidelines (such as dumping all your processing files in the top-level results directory) will be met with consequences.\nFilesystem overview Click on the image to view a larger version of the recommended directory structure or scroll down for another example in text.\n\nCreating a project directory using the template # TODO: Jess will create a project skeleton in the future  You should rename the directories starting with rename_* into something sensible.\nExample directory structure In this example, our project name is called project_name and is a RAD-seq experiment that was processed using Stacks.\nproject_name/ ├── data/ ├── results/ ├── scripts/ ├── software/ └── README  The directory has four subdirectories: data, results, scripts, and software, and one README file. The README file should be a plain-text file containing basic project information (e.g. what the project is about, what type of data was sequenced).\nData directory The data directory should contain the raw data received from sequencing. Each library should have it\u0026rsquo;s own directory containing sequencing files and a text file containing barcodes corresponding to samples. This file is needed for Stacks process_radtags.\nproject_name/ ├── data/ │ ├── library_1_raw_data/ │ │ ├── seqA_R1_001.fastq.gz │ │ ├── seqA_R2_001.fastq.gz │ │ └── library_1_barcodes.txt │ ├── library_2_raw_data/ │ │ ├── seqB_R1_001.fastq.gz │ │ ├── seqB_R2_001.fastq.gz │ │ └── library_2_barcodes.txt ├── results/ ├── scripts/ ├── software/ └── README  Make files read-only (optional) The files in your data directory should never be edited.\nIf you are familiar with UNIX file permissions, you can remove write permissions with the chmod command. For example, the following command removes write permission for all users:\nchmod a-w seqA_R1_001.fastq.gz  You can check file permissions with ls -l where the first column represents whether read/write/execute access is avaiable.\n$ ls -l -r--r--r-- 1 jess jess 142870 Aug 8 14:30 seqA_R1_001.fastq.gz -r--r--r-- 1 jess jess 177552 Aug 8 14:30 seqA_R2_001.fastq.gz  Results directory The results directory should have one directory for each time you generate a set of results. Using subdirectories inside the main results directory is recommended because often experiments are re-run in the future (e.g. updated software versions, more sequencing data, reanalysis before publication). I recommend naming the directory with a date in YYYY-MM-DD format at the beginning of the name so the directories are sorted chronologically.\nproject_name/ ├── data/ ├── results/ │ ├── 2017-08-01_results/ │ │ ├── ... │ │ ├── ... │ │ └── ... │ ├── 2017-11-01_extra_samples/ │ │ ├── ... │ │ ├── ... │ │ └── ... │ ├── 2018-05-01_reanalysis/ │ │ ├── ... │ │ ├── ... │ │ └── ... ├── scripts/ ├── software/ └── README  Inside each result subdirectory, there should be multiple directories containing output from steps in your workflow. In this example, the directories inside 2017-08-01_results are: demuxed_seq, demuxed_cat, alignments, stacks, and qc. Your directory names may look different depending on what type of analysis you\u0026rsquo;re performing. The contents of each directory is described below.\nSequencing data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ │ ├── mozzie-1.1.fq │ │ ├── mozzie-1.2.fq │ │ ├── mozzie-1.rem.1.fq │ │ ├── mozzie-1.rem.2.fq │ │ ├── mozzie-2.1.fq │ │ ├── mozzie-2.2.fq │ │ └── ... │ ├── demuxed_cat/ │ │ ├── mozzie-1.fq │ │ ├── mozzie-2.fq │ │ ├── mozzie-3.fq │ │ └── ... │ └── ...  The demuxed_seq directory contains demuxed sequencing data processed by process_ragtags. Stacks should output four files for each sample listed in the barcode file. In this example, mozzie-1.1.fq and mozzie-1.2.fq contain the set forward and reverse reads for the mozzie-1 sample. The mozzie-1.rem.1.fq and mozzie-1.rem.2.fq files contain the remaining reads that are unpaired due to their mate being discarded.\nIf you\u0026rsquo;re working with ddRADseq data, Stacks recommends concatenating the four files together. Here, demuxed_cat contains the concatenated files.\nAlignment data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ │ ├── AaegL2/ │ │ │ ├── mozzie-1.AaegL2.sorted.bam │ │ │ ├── mozzie-1.AaegL2.sorted.bam.bai │ │ │ ├── mozzie-2.AaegL2.sorted.bam │ │ │ ├── mozzie-2.AaegL2.sorted.bam.bai │ │ │ ├── ... │ │ │ └── AagL2_alignment_code.txt │ │ ├── AaegL3/ │ │ │ ├── mozzie-1.AaegL3.sorted.bam │ │ │ ├── mozzie-1.AaegL3.sorted.bam.bai │ │ │ ├── mozzie-2.AaegL3.sorted.bam │ │ │ ├── mozzie-2.AaegL3.sorted.bam.bai │ │ │ ├── ... │ │ │ └── AagL3_alignment_code.txt │ └── ...  Alignments should be stored in the alignments directory with a separate directory for each reference genome aligned against. Alignments should be stored as bam files with the .bam suffix and bam index files, if provided, should end with .bai. If alignements are sorted, it\u0026rsquo;s recommended to include sorted in the filename. Including the reference genome name in the filename is also helpful.\nA plain-text file with what commands were run should also be included in the directory (e.g. AagL2_alignment_code.txt) or in the scripts directory.\nStacks data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ ├── stacks/ │ │ ├── stacks_AaegL2_females/ │ │ │ ├── catalog/ │ │ │ │ ├── mozzie-1.AaegL2.alleles.tsv │ │ │ │ ├── mozzie-1.AaegL2.matches.tsv │ │ │ │ ├── mozzie-1.AaegL2.models.tsv │ │ │ │ ├── mozzie-1.AaegL2.snps.tsv │ │ │ │ ├── mozzie-2.AaegL2.alleles.tsv │ │ │ │ ├── ... │ │ │ │ ├── batch_1.catalog.alleles.tsv │ │ │ │ ├── batch_1.catalog.snps.tsv │ │ │ │ ├── batch_1.catalog.tags.tsv │ │ │ │ └── batch_1.markers.tsv │ │ │ ├── population_females_filtered/ │ │ │ │ ├── batch_1.vcf │ │ │ │ ├── batch_1.haplotypes.tsv │ │ │ │ ├── batch_1.sumstats_summary.tsv │ │ │ │ ├── batch_1.sumstats.tsv │ │ │ │ ├── batch_1.hapstats.tsv │ │ │ │ └── code_females_filtered.txt │ │ │ ├── population_females_singlesnp/ │ │ │ │ ├── batch_1.vcf │ │ │ │ ├── batch_1.haplotypes.tsv │ │ │ │ ├── batch_1.sumstats_summary.tsv │ │ │ │ ├── batch_1.sumstats.tsv │ │ │ │ ├── batch_1.hapstats.tsv │ │ │ │ └── code_females_singlesnp.txt │ │ ├── stacks_AaegL2_males/ │ │ │ └── ... │ │ └── ... │ └── ...  Each run of Stacks to get a catalogue should have its own separate directory in the stacks directory. The output files from ref_map or denovo_map stored in its own directory.\nEach time you run Stacks populations with designated filters, you should store the files in a separate directory. You should also include a file containing the code that was used to produce the output.\nQC data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ ├── stacks/ │ ├── qc/ │ │ ├── fastqc/ │ │ │ ├── ... │ │ │ ├── ... │ │ │ └── ... │ │ ├── flagstat/ │ │ │ ├── ... │ │ │ ├── ... │ │ │ └── ... │ │ └── ... │ └── ...  The qc directory should contain the output of programs run for quality control purposes (e.g. fastQC, samtools flagstat).\nScripts directory It\u0026rsquo;s up to you if you want to store your scripts inside the scripts directory or with the output files that were generated. Just make sure you document all the code that was run somewhere sensible.\nSoftware directory If you have any additional software you compiled specifically for your project, you can store them here.\nREADME files README files are plain-text files where you should write descriptions of what the directory contains, what analysis was done, why certain parameters were chosen, what results were found, etc. Place a README file in any directory you feel could use one. Documenting your work clearly is good practice and often pays dividends in the future.\n"
},
{
	"uri": "/pearg_documentation/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "  HPC Resources  An overview of high-performance computing resources available.\n Getting an Account  How to get an account on our clusters.\n Tips for Windows Users  Tips for Windows users starting out with UNIX.\n Getting Help  How to get technical help.\n "
},
{
	"uri": "/pearg_documentation/getting-started/hpc-resources/",
	"title": "HPC Resources",
	"tags": [],
	"description": "An overview of high-performance computing resources available.",
	"content": "This page lists the compute resources available to PEARG lab members.\n"
},
{
	"uri": "/pearg_documentation/metadata-documentation/using-mediaflux/",
	"title": "Using Mediaflux",
	"tags": [],
	"description": "How to use Mediaflux.",
	"content": "The Mediaflux service can be accessed via a Java applet (Mediaflux Explorer) or via your web browser with Mediaflux Desktop. These clients allow you to view your data, transfer data, and edit metadata.\nThe Mediaflux Explorer Java applet should be used when transferring data and is available on both mozzie and rescue servers. Note that uploading large files via the browser is unreliable and slow, therefore Mediaflux Desktop should not be used for uploading.\nAssuming you have your data stored on one of our servers (mozzie or rescue) open your preferred internet browser and enter the IP address of the server into the navigation bar.\nYou should be taken to the dashboard page that lists the services available. Find the \u0026lsquo;Lubuntu Desktop\u0026rsquo; service and click on the access link to use VNC. This allows us to access the server using a graphical interface through a browser.\nEnter your credentials for your account to login. You may need to do this twice\u0026mdash; once to access VNC and again to login to the Lubuntu desktop.\nLogging in will bring you to the Lubuntu desktop where you should see icons including a \u0026lsquo;Mediaflux\u0026rsquo; icon. Double click it to launch Mediaflux Explorer.\nSelect the HTTPS protocol in the dropdown menu and enter mediaflux.vicnode.org.au as the host. The port should automatically change to 443.\nIn the \u0026lsquo;Domain\u0026rsquo; field enter aaf and then move to the next field. A dropdown menu should then appear below the \u0026lsquo;Domain\u0026rsquo; field (if it doesn\u0026rsquo;t show up immediately, wait a few seconds). Select The University of Melbourne from the listed institutions.\nFinally, enter your unimelb username (not your email) and password to sign in.\n"
},
{
	"uri": "/pearg_documentation/best-practices/naming-files/",
	"title": "Naming Files",
	"tags": [],
	"description": "Best practices for naming files.",
	"content": " This document describes best practices for naming files.\nI highly recommend you read this Data Carpentary document on file naming.\nBe descriptive Good filenames should be precise. For example, a raw FASTQ filename received from AGRF may look something like this:\nX54321-1_AB1XXXXXX_GAATTCGT-ATAGAGGC_L007_R2.fastq.gz  The filename contains a lot of information which is descriptive to the user and can be easily extracted programatically.\nJust by looking at the filename and knowing what each field represents, we have the following information:\n Sample name: X54321-1 Flowcell ID: AB1XXXXXX Index: GAATTCGT-ATAGAGGC Lane: L007 Forward/Reverse: R2 File format: fastq Compression: gz  Storing metadata in this fashion allows us to avoid mixing up samples and losing expensive sequencing data due to lost information.\nBe consistent If you\u0026rsquo;re storing metadata in filenames, it\u0026rsquo;s good practice to be consistent with what each field represents. Being consisent also means filenames are sorted in a logical manner when listed alphabetically.\nGood consistency:\n2017-02-07_cg_meeting_notes_with_alice.md 2017-02-14_cg_meeting_notes_with_bob.md 2017-02-21_cg_meeting_notes_with_alice.md 2017-02-28_cg_meeting_notes_with_carol.md  Bad consistency:\n14.02.17_bob_cg_meeting.md 21-02-alice-meeting-cg.md carol_cg_meeting_notes_2017-02-28.md cg_meeting_with_alice_2017-02-07.md  Allowed characters Avoid special characters. The only characters you should be using in your filenames are the alphanumeric characters (A-Z, a-z, 0-9), underscores (_), periods (.), and dashes (-). Always start your filename with an alphanumeric character.\nAvoid using spaces Using spaces in filenames makes things difficult when working with UNIX. Therefore, use underscores _ and dashes - to separate words.\nFor example:\nsample-data-from-bob.txt 2017-03-03_raw_data_ID_A721BW.tar.gz  You can also use periods . for additional metadata information.\nsample-1.AaegL3.bam sample-1.AaegL3.sorted.bam sample-1.AaegL3.sorted.filtered.bam  YYYY-MM-DD The correct way to write numeric dates is YYYY-MM-DD, (i.e. ISO 8601).\nGood names:\n2017-08-01_results/ 2016-04-22_meeting_notes.txt 2018-11-29_raw-data-from-john-doe.tar.gz  Lowercase and uppercase Using purely lowecase letters, or a mixture of lower and uppercase letters in filenames are both ok. It\u0026rsquo;s a matter of personal preference.\nAvoid having files that differ only by case in the same directory. e.g.\nnotes.txt Notes.txt  "
},
{
	"uri": "/pearg_documentation/getting-started/getting-an-account/",
	"title": "Getting an Account",
	"tags": [],
	"description": "How to get an account on our clusters.",
	"content": "How to get an account on our clusters.\n"
},
{
	"uri": "/pearg_documentation/getting-started/tips-for-windows-users/",
	"title": "Tips for Windows Users",
	"tags": [],
	"description": "Tips for Windows users starting out with UNIX.",
	"content": "This document will include helpful things to know such as ^M line-endings, SSH clients, transferring files to and from remote servers, etc.\n"
},
{
	"uri": "/pearg_documentation/usage-documentation/reference-data/",
	"title": "Reference Data",
	"tags": [],
	"description": "Organisation of shared reference data.",
	"content": "This document describes how the reference data will be stored.\n"
},
{
	"uri": "/pearg_documentation/best-practices/stacks-databases/",
	"title": "Stacks Databases",
	"tags": [],
	"description": "Best practises for working with Stacks databases.",
	"content": " # # Note: This document is a work in progress and the commands are untested #  Database management Databases are usually large in size and we have limited space on our clusters. To remedy this, please remove any databases you don\u0026rsquo;t need. If you have a database older than 3 months in the system, you will be asked to clean up your old databases.\nNaming your databases Your database name must end with _radtags to show up in the web interface.\nMake sure to include your name and the date your database was created in your database name. Some examples of good database names are:\nalice_mozzie_20170304_radtags bob_possum_filtered_20170701_radtags carol_AaegL2_males_20170122_radtags  If your name and a creation date is not included in your database name, your database may be deleted without warning.\n Removing databases Delete databases by using the SQL DROP DATABASE statement.\nmysql -e \u0026quot;DROP DATABASE jess_20170815_radtags\u0026quot;  Backing up databases You can also backup your database with the mysqldump command. The output is in plain-text, so use gzip to compress the file to save space.\nmysqldump --databases jess_20170815_radtags \\ | gzip \\ \u0026gt; ~/my_backups/jess_20170815_radtags.sql.gz  Restoring a database from backup You can restore your saved databases by importing your file into MySQL.\n# Uncompress your file gunzip jess_20170815_radtags.sql.gz # Import from file mysql jess_20170815_radtags \u0026lt; ~/my_backups/jess_20170815_radtags.sql  Running Stacks on clusters which don\u0026rsquo;t allow databases If you\u0026rsquo;re using the barcoo cluster for your analysis, you won\u0026rsquo;t be able to load your catalog to a database on the cluster. This means you\u0026rsquo;ll need to do your computation (on barcoo) separately to a machine that alows database access.\nOn barcoo If you\u0026rsquo;re using ref_map.pl or denovo_map.pl you can use the -S option to disable all database interaction. The output from this step should be many tsv files.\nExample output:\nbatch_1.catalog.alleles.tsv batch_1.catalog.snps.tsv batch_1.catalog.tags.tsv batch_1.haplotypes.tsv batch_1.hapstats.tsv batch_1.markers.tsv batch_1.populations.log batch_1.sumstats_summary.tsv batch_1.sumstats.tsv batch_1.vcf mozzie-1.alleles.tsv mozzie-1.matches.tsv mozzie-1.models.tsv mozzie-1.snps.tsv mozzie-1.tags.tsv mozzie-2.alleles.tsv mozzie-2.matches.tsv mozzie-2.models.tsv mozzie-2.snps.tsv mozzie-2.tags.tsv ...  Transfer your data You\u0026rsquo;ll need to transfer your output files from ref_map.pl or denovo_map.pl to another computer which you have permission to create databases on. Since tsv files are plain-text, it\u0026rsquo;s good practice to compress them before transferring.\nYou can use a graphical FTP client or the command line to transfer files. Here we\u0026rsquo;ll use rsync with the --compress option to do transfer the files.\nOn barcoo, in an interactive slurm session, you can transfer the directory called catalog with the following command:\nrsync -av --compress --update --progress \\ catalog \\ \u0026lt;username\u0026gt;@\u0026lt;IP-address\u0026gt;:\u0026lt;path_to_my_project_directory\u0026gt;  replacing \u0026lt;username\u0026gt;, \u0026lt;IP-address\u0026gt;, and \u0026lt;path_to_my_project_directory\u0026gt; with your relevant information.\nLoad the catalog with load_radtags.pl On your machine which you have permission to create databases, load your catalog with load_radtags.pl.\n# Create an empty database mysql -e \u0026quot;CREATE DATABASE jess_20170815_radtags\u0026quot; # Load template tables mysql jess_20170815_radtags \u0026lt; /usr/local/share/stacks/sql/stacks.sql # Load RAD tags load_radtags.pl \\ -D jess_20170815_radtags \\ -p catalog \\ -b 1 -B -e \u0026quot;Description here\u0026quot; \\ -c  "
},
{
	"uri": "/pearg_documentation/best-practices/backing-up-your-data/",
	"title": "Backing Up Your Data",
	"tags": [],
	"description": "Best practices for backing up your data.",
	"content": " If you only have a single copy of your data, you\u0026rsquo;re vulnerable to losing it. A hardware failure or theft can lead to losing hundreds of hours of work or irreplacable photos.\n\u0026ldquo;Don\u0026rsquo;t worry\u0026mdash; I have a Time Machine backup,\u0026rdquo; I hear you say. Having a Time Machine backup is a good first step, however, you\u0026rsquo;re still at risk in cases where you lose both copies in a house fire or if both your laptop and Time Machine hard drive are stolen.\nHard disk drives are also prone to failure.\n\nOne solution is to store a copy of your data remotely in the cloud. Storage services will also have redundancy measures in place to avoid data loss when hard drives inevitably fail.\nA good rule to follow is the 3-2-1 rule for backing up your data.\nThe 3-2-1 rule for backups The 3-2-1 rule for backup goes as follows:\n Have at least three copies of your data Use at least two different types of media for your copies At least one of your copies should be offsite    Backing up your experimental data Following the 3-2-1 rule, your sequencing data for your study should have at least three copies. A typical case would be:\n one copy on a external hard drive stored in the lab one copy on the volume attached to the server where you\u0026rsquo;re doing your analysis one copy in Mediaflux labeled with appropriate metadata  Cloud-based automated backup services Having a remote backup for your computer is also a good idea. Services such as Backblaze can automatically backup your computer, but they also have a monthly subscription fee at around $5 a month.\nAn example of using the 3-2-1 rule for backing up your laptop would be:\n one copy on the SSD in your laptop one copy on a HDD with Time Machine backups one copy in the cloud using a cloud backup service such as Backblaze  "
},
{
	"uri": "/pearg_documentation/getting-started/getting-help/",
	"title": "Getting Help",
	"tags": [],
	"description": "How to get technical help.",
	"content": "Getting help.\n"
},
{
	"uri": "/pearg_documentation/best-practices/",
	"title": "Best Practices",
	"tags": [],
	"description": "",
	"content": "  Directory Structure  How to structure your project directory when performing an analysis.\n Naming Files  Best practices for naming files.\n Stacks Databases  Best practises for working with Stacks databases.\n Backing Up Your Data  Best practices for backing up your data.\n "
},
{
	"uri": "/pearg_documentation/metadata-documentation/",
	"title": "Metadata Documentation",
	"tags": [],
	"description": "",
	"content": "  Metadata Overview  A overview of our metadata system in use.\n Using Mediaflux  How to use Mediaflux.\n "
},
{
	"uri": "/pearg_documentation/usage-documentation/",
	"title": "Usage Documentation",
	"tags": [],
	"description": "",
	"content": "  Running Jobs with SLURM  Documentation on how to run SLURM jobs.\n Reference Data  Organisation of shared reference data.\n "
},
{
	"uri": "/pearg_documentation/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "PEARG Documentation\n"
},
{
	"uri": "/pearg_documentation/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/pearg_documentation/",
	"title": "PEARG Documentation",
	"tags": [],
	"description": "Bioinformatics documentation for PEARG lab members",
	"content": " PEARG Documentation This site contains bioinformatics-related documentation for PEARG lab members.\nGetting started These documents go over what compute resources are available to you, how to get an account on our clusters, and some terms of conditions.\n HPC resources overview Getting an account Tips for Windows users  Best practices These documents describe some best practices when using our compute resources. Not following some of these guidelines may result in a scolding.\n Directory structure Naming files Stacks databases  Metadata Documentation These documents describe how to use our metadata system to record information relating to your samples.\n Metadata overview Using Mediaflux  Usage Documentation Documentation related to using our clusters.\n Running jobs with SLURM  Reference data  Contribute to this documentation This documentation\u0026rsquo;s source code is hosted on GitHub. Feel free to update the content\u0026mdash; just click the Edit this page link displayed on top right of each page, and create a pull request when done.\nContact Contact me at jchung@unimelb.edu.au or submit an issue on GitHub.\n"
},
{
	"uri": "/pearg_documentation/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]