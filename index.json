[
{
	"uri": "/pearg_documentation/metadata-documentation/metadata-overview/",
	"title": "Metadata Overview",
	"tags": [],
	"description": "A overview of our metadata system in use.",
	"content": "This document describes our system of storing metadata for sequencing samples.\n"
},
{
	"uri": "/pearg_documentation/usage-documentation/running-jobs-with-slurm/",
	"title": "Running Jobs with SLURM",
	"tags": [],
	"description": "Documentation on how to run SLURM jobs.",
	"content": " Using a computer cluster with other users means sharing resources. SLURM (Simple Linux Utility for Resource Management) is a commonly used job scheduler that manages a queue where you submit your jobs and allocates resources to run your job when resources are available.\nThe documentation on using SLURM at Melbourne Bioinformatics is quite comprehensive and can be found here.\nChecking the status of your jobs squeue You can view information about jobs in the SLURM queue with the squeue command. View the help message with squeue --usage or the manual with man squeue.\n# List all jobs in the queue squeue # List all jobs for account UOM0041 squeue --account=UOM0041 # List all jobs in the queue for user jchung in long format squeue -l -u jchung  If you\u0026rsquo;re using snowy or barcoo, you can also use the showq command.\nscontrol show job If you want information regarding a specific job id, you can use scontrol.\nscontrol show job \u0026lt;job-id\u0026gt;  sacct You can check the status of recently finished jobs with sacct.\nsacct  sinfo You can also view the status of the nodes in the cluster with sinfo.\n# Show status of all nodes sinfo -Nel  If you want more information about a specific node, you can use scontrol.\n# View information on the master node scontrol show node master  Running your jobs sbatch Most of your jobs will be sumbitted to SLURM via sbatch. The commands that you want to run need to be written in a script (a plain-text file that we\u0026rsquo;ll discuss further below), saved to a location, then submitted using sbatch.\n# Print the help message from sbatch sbatch --help # Submit your script by specifying the name of your script sbatch my-script.sh  sinteractive You can use the sinteractive command to run your job in an interactive session. When SLURM allocates your job resources, you will be provided with an interactive terminal session. It is recommended to use sinteractive in conjunction with a terminal multiplexer such as GUN Screen so the job won\u0026rsquo;t terminate if you disconnect from the server.\n# Print the help message for sinteractive sinteractive --help # Submit a job with the default parameters sinteractive # Submit a job with 4 CPUs, 16 GB memory, and wall time of 1 day sinteractive --ntasks=1 --cpu-per-task=4 --mem=16384 --time=1-0:0:0  Note that the memory amount is specified in MB.\nscancel You can cancel a running job or a job in the queue with scancel.\nscancel \u0026lt;job-id\u0026gt;  Writing a SLURM script For beginners, I recommend using the job script generator written by Melbourne Bioinformatics. If you\u0026rsquo;re using one of the PEARG clusters on the Nectar cloud (i.e. mozzie or rescue), you can ignore the \u0026ldquo;Project ID\u0026rdquo; and the \u0026ldquo;Modules\u0026rdquo; field.\nHere\u0026rsquo;s an example of a simple SLURM script running on the mozzie server.\n#!/bin/bash #SBATCH --job-name=denovo_map #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 denovo_map.pl \\ -m 3 -M 2 -n 1 -T 8 -b 1 -S \\ -o denovo_map_m3_M2_n1_2017-11-09 \\ -s ../processed_radtags/sample-1.fastq.gz \\ -s ../processed_radtags/sample-2.fastq.gz \\ -X \u0026quot;populations: --vcf\u0026quot;  The first line of the script must specify the interpreter the script will be executed with such as bash or sh. Keep this as #!/bin/bash unless you have reason to change it.\nEach line starting with #SBATCH is an option that SLURM\u0026rsquo;s sbatch command uses. You can get view all available options with sbatch -h or by viewing the man page with man sbatch.\nThe most common #SBATCH options you\u0026rsquo;ll most likely be using are:\n --job-name=XXX: You should always specify a job name for your job --nodes=1: In most cases, you should be requesting one node so all the requested CPUs are on the same node. --ntasks=1: In most cases you\u0026rsquo;ll be running one task per job --cpus-per-task=X: Specify the number of CPUs to request.  You can also direct your stdout and stderr into defined files:\n -output my-file-%j.out -error my-file-%j.err  If you\u0026rsquo;re using barcoo or snowy, you\u0026rsquo;ll also need to specify memory in MB with:\n --mem=XXXXX for jobs using multiple CPUs, or --mem-per-cpu=XXXX for single CPU jobs.  With barcoo and snowy, you\u0026rsquo;ll also need to specify the partition, and a time limit:\n -p main: The partition is called \u0026lsquo;main\u0026rsquo; --time=D-HH:MM:SS: Time limit given for the job. If the job exceeds the time, it is automatically terminated.  Here\u0026rsquo;s an example of a SLURM script for barcoo.\n#!/bin/bash # Partition for the job: #SBATCH -p main # Account to run the job: #SBATCH --account=VR0002 # Multithreaded (SMP) job: must run on one node #SBATCH --nodes=1 # The name of the job: #SBATCH --job-name=\u0026quot;test-job\u0026quot; # Maximum number of tasks/CPU cores used by the job: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 # The amount of memory in megabytes per process in the job: #SBATCH --mem=32768 # The maximum running time of the job in days-hours:mins:sec #SBATCH --time=0-1:0:00 # Run the job from your home directory: cd $HOME # The job command(s): sleep 10  When using barcoo and snowy, don\u0026rsquo;t forget to module load the software you need into your environment path.\n"
},
{
	"uri": "/pearg_documentation/best-practices/directory-structure/",
	"title": "Directory Structure",
	"tags": [],
	"description": "How to structure your project directory when performing an analysis.",
	"content": " When using the PEARG clusters or the Melbourne Bioinformatics clusters, your files and data should be organised according to the convention described here.\nEach project should have its own directory located inside your home directory or a shared location. Your project directory name should be meaningful.\nAny shared data such as reference genomes and indices should be stored outside your project directory to avoid unnecessary duplication of data.\nCheck your project directory At absolute minimum, please check:\n is your project name sensible? do you have a README file (or equivalent) in the parent directory of your project directory that contains meaningful information? do you have a data directory (containing your raw data) and a results directory (containing processed data) with files organised in a logical manner?  If you\u0026rsquo;ve answered no to any of these questions, you may get a stern email in the future.\n  The motivation behind having a strict structure that all members adhere to for project organisation is transparency. Anyone from the lab should be able to look in your project directory and clearly understand what was run and reproduce your analysis. Most of the time, the person trying to decode what analyses were performed and why will be future you. Cooperate with your future self by leaving verbose notes in README files!\nWhile the sysadmin won\u0026rsquo;t be authoritarian about the precise directory structure, any flagrant disregard for the guidelines (such as dumping all your processing files in the top-level results directory) will be met with consequences.\nFilesystem overview Click on the image to view a larger version of the recommended directory structure or scroll down for another example in text.\n\nCreating a project directory using the template # TODO: Jess will create a project skeleton in the future  You should rename the directories starting with rename_* into something sensible.\nExample directory structure In this example, our project name is called project_name and is a RAD-seq experiment that was processed using Stacks.\nproject_name/ ├── data/ ├── results/ ├── scripts/ ├── software/ └── README  The directory has four subdirectories: data, results, scripts, and software, and one README file. The README file should be a plain-text file containing basic project information (e.g. what the project is about, what type of data was sequenced).\nData directory The data directory should contain the raw data received from sequencing. Each library should have it\u0026rsquo;s own directory containing sequencing files and a text file containing barcodes corresponding to samples. This file is needed for Stacks process_radtags.\nproject_name/ ├── data/ │ ├── library_1_raw_data/ │ │ ├── seqA_R1_001.fastq.gz │ │ ├── seqA_R2_001.fastq.gz │ │ └── library_1_barcodes.txt │ ├── library_2_raw_data/ │ │ ├── seqB_R1_001.fastq.gz │ │ ├── seqB_R2_001.fastq.gz │ │ └── library_2_barcodes.txt ├── results/ ├── scripts/ ├── software/ └── README  Make files read-only (optional) The files in your data directory should never be edited.\nIf you are familiar with UNIX file permissions, you can remove write permissions with the chmod command. For example, the following command removes write permission for all users:\nchmod a-w seqA_R1_001.fastq.gz  You can check file permissions with ls -l where the first column represents whether read/write/execute access is avaiable.\n$ ls -l -r--r--r-- 1 jess jess 142870 Aug 8 14:30 seqA_R1_001.fastq.gz -r--r--r-- 1 jess jess 177552 Aug 8 14:30 seqA_R2_001.fastq.gz  Results directory The results directory should have one directory for each time you generate a set of results. Using subdirectories inside the main results directory is recommended because often experiments are re-run in the future (e.g. updated software versions, more sequencing data, reanalysis before publication). I recommend naming the directory with a date in YYYY-MM-DD format at the beginning of the name so the directories are sorted chronologically.\nproject_name/ ├── data/ ├── results/ │ ├── 2017-08-01_results/ │ │ ├── ... │ │ ├── ... │ │ └── ... │ ├── 2017-11-01_extra_samples/ │ │ ├── ... │ │ ├── ... │ │ └── ... │ ├── 2018-05-01_reanalysis/ │ │ ├── ... │ │ ├── ... │ │ └── ... ├── scripts/ ├── software/ └── README  Inside each result subdirectory, there should be multiple directories containing output from steps in your workflow. In this example, the directories inside 2017-08-01_results are: demuxed_seq, demuxed_cat, alignments, stacks, and qc. Your directory names may look different depending on what type of analysis you\u0026rsquo;re performing. The contents of each directory is described below.\nSequencing data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ │ ├── mozzie-1.1.fq │ │ ├── mozzie-1.2.fq │ │ ├── mozzie-1.rem.1.fq │ │ ├── mozzie-1.rem.2.fq │ │ ├── mozzie-2.1.fq │ │ ├── mozzie-2.2.fq │ │ └── ... │ ├── demuxed_cat/ │ │ ├── mozzie-1.fq │ │ ├── mozzie-2.fq │ │ ├── mozzie-3.fq │ │ └── ... │ └── ...  The demuxed_seq directory contains demuxed sequencing data processed by process_ragtags. Stacks should output four files for each sample listed in the barcode file. In this example, mozzie-1.1.fq and mozzie-1.2.fq contain the set forward and reverse reads for the mozzie-1 sample. The mozzie-1.rem.1.fq and mozzie-1.rem.2.fq files contain the remaining reads that are unpaired due to their mate being discarded.\nIf you\u0026rsquo;re working with ddRADseq data, Stacks recommends concatenating the four files together. Here, demuxed_cat contains the concatenated files.\nAlignment data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ │ ├── AaegL2/ │ │ │ ├── mozzie-1.AaegL2.sorted.bam │ │ │ ├── mozzie-1.AaegL2.sorted.bam.bai │ │ │ ├── mozzie-2.AaegL2.sorted.bam │ │ │ ├── mozzie-2.AaegL2.sorted.bam.bai │ │ │ ├── ... │ │ │ └── AagL2_alignment_code.txt │ │ ├── AaegL3/ │ │ │ ├── mozzie-1.AaegL3.sorted.bam │ │ │ ├── mozzie-1.AaegL3.sorted.bam.bai │ │ │ ├── mozzie-2.AaegL3.sorted.bam │ │ │ ├── mozzie-2.AaegL3.sorted.bam.bai │ │ │ ├── ... │ │ │ └── AagL3_alignment_code.txt │ └── ...  Alignments should be stored in the alignments directory with a separate directory for each reference genome aligned against. Alignments should be stored as bam files with the .bam suffix and bam index files, if provided, should end with .bai. If alignements are sorted, it\u0026rsquo;s recommended to include sorted in the filename. Including the reference genome name in the filename is also helpful.\nA plain-text file with what commands were run should also be included in the directory (e.g. AagL2_alignment_code.txt) or in the scripts directory.\nStacks data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ ├── stacks/ │ │ ├── stacks_AaegL2_females/ │ │ │ ├── catalog/ │ │ │ │ ├── mozzie-1.AaegL2.alleles.tsv │ │ │ │ ├── mozzie-1.AaegL2.matches.tsv │ │ │ │ ├── mozzie-1.AaegL2.models.tsv │ │ │ │ ├── mozzie-1.AaegL2.snps.tsv │ │ │ │ ├── mozzie-2.AaegL2.alleles.tsv │ │ │ │ ├── ... │ │ │ │ ├── batch_1.catalog.alleles.tsv │ │ │ │ ├── batch_1.catalog.snps.tsv │ │ │ │ ├── batch_1.catalog.tags.tsv │ │ │ │ └── batch_1.markers.tsv │ │ │ ├── population_females_filtered/ │ │ │ │ ├── batch_1.vcf │ │ │ │ ├── batch_1.haplotypes.tsv │ │ │ │ ├── batch_1.sumstats_summary.tsv │ │ │ │ ├── batch_1.sumstats.tsv │ │ │ │ ├── batch_1.hapstats.tsv │ │ │ │ └── code_females_filtered.txt │ │ │ ├── population_females_singlesnp/ │ │ │ │ ├── batch_1.vcf │ │ │ │ ├── batch_1.haplotypes.tsv │ │ │ │ ├── batch_1.sumstats_summary.tsv │ │ │ │ ├── batch_1.sumstats.tsv │ │ │ │ ├── batch_1.hapstats.tsv │ │ │ │ └── code_females_singlesnp.txt │ │ ├── stacks_AaegL2_males/ │ │ │ └── ... │ │ └── ... │ └── ...  Each run of Stacks to get a catalogue should have its own separate directory in the stacks directory. The output files from ref_map or denovo_map stored in its own directory.\nEach time you run Stacks populations with designated filters, you should store the files in a separate directory. You should also include a file containing the code that was used to produce the output.\nQC data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ ├── stacks/ │ ├── qc/ │ │ ├── fastqc/ │ │ │ ├── ... │ │ │ ├── ... │ │ │ └── ... │ │ ├── flagstat/ │ │ │ ├── ... │ │ │ ├── ... │ │ │ └── ... │ │ └── ... │ └── ...  The qc directory should contain the output of programs run for quality control purposes (e.g. fastQC, samtools flagstat).\nScripts directory It\u0026rsquo;s up to you if you want to store your scripts inside the scripts directory or with the output files that were generated. Just make sure you document all the code that was run somewhere sensible.\nSoftware directory If you have any additional software you compiled specifically for your project, you can store them here.\nREADME files README files are plain-text files where you should write descriptions of what the directory contains, what analysis was done, why certain parameters were chosen, what results were found, etc. Place a README file in any directory you feel could use one. Documenting your work clearly is good practice and often pays dividends in the future.\n"
},
{
	"uri": "/pearg_documentation/getting-started/hpc-resources/",
	"title": "Compute Resources",
	"tags": [],
	"description": "An overview of high-performance computing resources available.",
	"content": " This page lists the compute resources available to PEARG lab members.\nCloud Servers These clusters are hosted on the Nectar cloud. Email jchung@unimelb.edu.au to request an account.\nWhen using these servers you should follow the best practices described in these documents.\nEach server has approximately 10 TB of storage space which is not backed up. It is your own responsibility to have backups of your important data.\nMosquito Server (mozzie) IP address: 45.113.232.220\nThe mozzie server is a 32 core cluster comprised of two 16-core machines. The master node has 15 cores available for use and 64 GB of RAM, while the worker node has 16 cores and 128 GB available.\nGenetic Rescue Server (rescue) IP address: 115.146.85.115\nThe rescue server is a 28 core cluster comprised of two machines. The master node has 11 cores available for use and 48 GB of RAM, while the worker node has 16 cores and 128 GB availble.\nMelbourne Bioinformatics HPC Projects can also apply to get access to the Melbourne Bioinformatics clusters, barcoo and snowy which have hundreds of cores and a few nodes with very high amounts of memory.\nThe documentation for using the Melbourne Bioinformatics computational resources are located here.\n"
},
{
	"uri": "/pearg_documentation/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "  Compute Resources  An overview of high-performance computing resources available.\n Getting an Account  How to get an account on our clusters.\n Tips for Windows Users  Tips for Windows users starting out with UNIX.\n Getting Help  How to get technical help.\n "
},
{
	"uri": "/pearg_documentation/metadata-documentation/using-mediaflux/",
	"title": "Using Mediaflux",
	"tags": [],
	"description": "How to use Mediaflux.",
	"content": "The Mediaflux service can be accessed via a Java applet (Mediaflux Explorer) or via your web browser with Mediaflux Desktop. These clients allow you to view your data, transfer data, and edit metadata.\nThe Mediaflux Explorer Java applet should be used when transferring data and is available on both mozzie and rescue servers. Note that uploading large files via the browser is unreliable and slow, therefore Mediaflux Desktop should not be used for uploading.\nAssuming you have your data stored on one of our servers (mozzie or rescue) open your preferred internet browser and enter the IP address of the server into the navigation bar.\nYou should be taken to the dashboard page that lists the services available. Find the \u0026lsquo;Lubuntu Desktop\u0026rsquo; service and click on the access link to use VNC. This allows us to access the server using a graphical interface through a browser.\nEnter your credentials for your account to login. You may need to do this twice\u0026mdash; once to access VNC and again to login to the Lubuntu desktop.\nLogging in will bring you to the Lubuntu desktop where you should see icons including a \u0026lsquo;Mediaflux\u0026rsquo; icon. Double click it to launch Mediaflux Explorer.\nSelect the HTTPS protocol in the dropdown menu and enter mediaflux.vicnode.org.au as the host. The port should automatically change to 443.\nIn the \u0026lsquo;Domain\u0026rsquo; field enter aaf and then move to the next field. A dropdown menu should then appear below the \u0026lsquo;Domain\u0026rsquo; field (if it doesn\u0026rsquo;t show up immediately, wait a few seconds). Select The University of Melbourne from the listed institutions.\nFinally, enter your unimelb username (not your email) and password to sign in.\n"
},
{
	"uri": "/pearg_documentation/best-practices/naming-files/",
	"title": "Naming Files",
	"tags": [],
	"description": "Best practices for naming files.",
	"content": " This document describes best practices for naming files.\nI highly recommend you read this Data Carpentary document on file naming.\nBe descriptive Good filenames should be precise. For example, a raw FASTQ filename received from AGRF may look something like this:\nX54321-1_AB1XXXXXX_GAATTCGT-ATAGAGGC_L007_R2.fastq.gz  The filename contains a lot of information which is descriptive to the user and can be easily extracted programatically.\nJust by looking at the filename and knowing what each field represents, we have the following information:\n Sample name: X54321-1 Flowcell ID: AB1XXXXXX Index: GAATTCGT-ATAGAGGC Lane: L007 Forward/Reverse: R2 File format: fastq Compression: gz  Storing metadata in this fashion allows us to avoid mixing up samples and losing expensive sequencing data due to lost information.\nBe consistent If you\u0026rsquo;re storing metadata in filenames, it\u0026rsquo;s good practice to be consistent with what each field represents. Being consisent also means filenames are sorted in a logical manner when listed alphabetically.\nGood consistency:\n2017-02-07_cg_meeting_notes_with_alice.md 2017-02-14_cg_meeting_notes_with_bob.md 2017-02-21_cg_meeting_notes_with_alice.md 2017-02-28_cg_meeting_notes_with_carol.md  Bad consistency:\n14.02.17_bob_cg_meeting.md 21-02-alice-meeting-cg.md carol_cg_meeting_notes_2017-02-28.md cg_meeting_with_alice_2017-02-07.md  Allowed characters Avoid special characters. The only characters you should be using in your filenames are the alphanumeric characters (A-Z, a-z, 0-9), underscores (_), periods (.), and dashes (-). Always start your filename with an alphanumeric character.\nAvoid using spaces Using spaces in filenames makes things difficult when working with UNIX. Therefore, use underscores _ and dashes - to separate words.\nFor example:\nsample-data-from-bob.txt 2017-03-03_raw_data_ID_A721BW.tar.gz  You can also use periods . for additional metadata information.\nsample-1.AaegL3.bam sample-1.AaegL3.sorted.bam sample-1.AaegL3.sorted.filtered.bam  YYYY-MM-DD The correct way to write numeric dates is YYYY-MM-DD, (i.e. ISO 8601).\nGood names:\n2017-08-01_results/ 2016-04-22_meeting_notes.txt 2018-11-29_raw-data-from-john-doe.tar.gz  Lowercase and uppercase Using purely lowecase letters, or a mixture of lower and uppercase letters in filenames are both ok. It\u0026rsquo;s a matter of personal preference.\nAvoid having files that differ only by case in the same directory. e.g.\nnotes.txt Notes.txt  "
},
{
	"uri": "/pearg_documentation/usage-documentation/stacks-databases/",
	"title": "Stacks Databases",
	"tags": [],
	"description": "Best practises for working with Stacks databases.",
	"content": " Database management Databases are usually large in size and we have limited space on our clusters. To remedy this, please remove any databases you don\u0026rsquo;t need. If you have a database older than 3 months in the system, you will be asked to clean up your old databases.\nNaming your databases Your database name must end with _radtags to show up in the web interface.\nMake sure to include your name and the date your database was created in your database name. Some examples of good database names are:\nalice_mozzie_20170304_radtags bob_possum_filtered_20170701_radtags carol_AaegL2_males_20170122_radtags  If your name and a creation date is not included in your database name, your database may be deleted without warning.\n Removing databases Delete databases by using the SQL DROP DATABASE statement.\nmysql -e \u0026quot;DROP DATABASE jess_20170815_radtags\u0026quot;  Backing up databases You can also backup your database with the mysqldump command. The output is in plain-text, so use gzip to compress the file to save space.\nmysqldump --databases jess_20170815_radtags \\ | gzip \\ \u0026gt; ~/my_backups/jess_20170815_radtags.sql.gz  Restoring a database from backup You can restore your saved databases by importing your file into MySQL.\n# Uncompress your file gunzip jess_20170815_radtags.sql.gz # Import from file mysql jess_20170815_radtags \u0026lt; ~/my_backups/jess_20170815_radtags.sql  Running Stacks on clusters which don\u0026rsquo;t allow databases If you\u0026rsquo;re using the barcoo cluster for your analysis, you won\u0026rsquo;t be able to load your catalog to a database on the cluster. This means you\u0026rsquo;ll need to do your computation (on barcoo) separately to a machine that alows database access.\nOn barcoo If you\u0026rsquo;re using ref_map.pl or denovo_map.pl you can use the -S option to disable all database interaction. The output from this step should be many tsv files.\nExample output:\nbatch_1.catalog.alleles.tsv batch_1.catalog.snps.tsv batch_1.catalog.tags.tsv batch_1.haplotypes.tsv batch_1.hapstats.tsv batch_1.markers.tsv batch_1.populations.log batch_1.sumstats_summary.tsv batch_1.sumstats.tsv batch_1.vcf mozzie-1.alleles.tsv mozzie-1.matches.tsv mozzie-1.models.tsv mozzie-1.snps.tsv mozzie-1.tags.tsv mozzie-2.alleles.tsv mozzie-2.matches.tsv mozzie-2.models.tsv mozzie-2.snps.tsv mozzie-2.tags.tsv ...  Transfer your data You\u0026rsquo;ll need to transfer your output files from ref_map.pl or denovo_map.pl to another computer which you have permission to create databases on. Since tsv files are plain-text, it\u0026rsquo;s good practice to compress them before transferring.\nYou can use a graphical FTP client or the command line to transfer files. Here we\u0026rsquo;ll use rsync with the --compress option to do transfer the files.\nOn barcoo, in an interactive slurm session, you can transfer the directory called catalog with the following command:\nrsync -av --compress --update --progress \\ catalog \\ \u0026lt;username\u0026gt;@\u0026lt;IP-address\u0026gt;:\u0026lt;path_to_my_project_directory\u0026gt;  replacing \u0026lt;username\u0026gt;, \u0026lt;IP-address\u0026gt;, and \u0026lt;path_to_my_project_directory\u0026gt; with your relevant information.\nLoad the catalog with load_radtags.pl On your machine which you have permission to create databases, load your catalog with load_radtags.pl.\n# Create an empty database mysql -e \u0026quot;CREATE DATABASE jess_20170815_radtags\u0026quot; # Load template tables mysql jess_20170815_radtags \u0026lt; /mnt/galaxy/gvl/software/stacks/share/stacks/sql/stacks.sql # Load RAD tags load_radtags.pl \\ -D jess_20170815_radtags \\ -p catalog \\ -b 1 -B -e \u0026quot;Description here\u0026quot; \\ -c  Index the catalog with index_radtags.pl You\u0026rsquo;ll then need to create indices for your database. Use -c to generate the catalog index and -d to generate the unique tags index.\nindex_radtags.pl \\ -D jess_20170815_radtags \\ -c \\ -t  View your data in the web interface With your web browser, go to the server you have your data on, and navigate to the Stacks page. Click on your database name to view it.\n"
},
{
	"uri": "/pearg_documentation/getting-started/getting-an-account/",
	"title": "Getting an Account",
	"tags": [],
	"description": "How to get an account on our clusters.",
	"content": "To obtain an account on mozzie or rescue, email jchung@unimelb.edu.au and request an account.\nAfter you SSH into the server, the first thing you should do is change your password. You can do this with the passwd command and follow the prompts.\n"
},
{
	"uri": "/pearg_documentation/getting-started/tips-for-windows-users/",
	"title": "Tips for Windows Users",
	"tags": [],
	"description": "Tips for Windows users starting out with UNIX.",
	"content": "This document will include helpful things to know such as ^M line-endings, SSH clients, transferring files to and from remote servers, etc.\n"
},
{
	"uri": "/pearg_documentation/usage-documentation/reference-data/",
	"title": "Reference Data",
	"tags": [],
	"description": "Organisation of shared reference data.",
	"content": "This document describes how the reference data will be stored.\n"
},
{
	"uri": "/pearg_documentation/usage-documentation/installing-software/",
	"title": "Installing Software",
	"tags": [],
	"description": "How to install software on our servers.",
	"content": " If you want a particular piece of software installed on mozzie or rescue you can email me at jchung@unimelb.edu.au. Anyone is also free to compile and install software in their personal directories.\nInstalling software on Linux can be challenging. Sometimes it\u0026rsquo;s as easy as a downloading a pre-compiled binary file and sometimes you\u0026rsquo;re stuck in Dependency Hell. There are also be many different ways to install software depending on which form it comes in. The most common ways you should use listed below.\nDownloading binaries or jar archives If the software is pre-compiled or is a java archive (ending in *.jar), you can just download it to your directory and run it.\nFor example, let\u0026rsquo;s say you want to install the latest version of Bowtie2 because the bowtie already installed on the cluster doesn\u0026rsquo;t have the new feature you want.\nYou would usually search for the software in your favourite search engine, then make your way to the download page. Bowtie2 is hosted on Sourceforge and has pre-compiled binaries, so we\u0026rsquo;ll download the software from there.\nThe Bowtie2 2.3.3 directory on Sourceforge currently has three items:\n bowtie2-2.3.3-macos-x86_64.zip bowtie2-2.3.3-linux-x86_64.zip bowtie2-2.3.3-source.zip  The first two items contain binaries for MacOS and Linux respectively. The \u0026ldquo;x86_64\u0026rdquo;refers to the architecture it\u0026rsquo;s meant to run on. The last file is an archive containing the source code if we want to compile it ourselves.\nSince our cluster runs Linux, we can download bowtie2-2.3.3-linux-x86_64.zip. Right click on the link and copy the link address so we can download it to the server.\n# Login and change to a directory where you want to store your software cd software # Download the file wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.3/bowtie2-2.3.3-linux-x86_64.zip/download -O bowtie2-2.3.3-linux-x86_64.zip # Unzip the directory unzip bowtie2-2.3.3-linux-x86_64.zip # Enter the directory and see what's inside cd bowtie2-2.3.3 \u0026amp;\u0026amp; ls # Run and view the help options of bowtie ./bowtie2 --help  Java archives are also simple to run. Just download the .jar file, and execute something like:\njava -jar \u0026lt;my_program\u0026gt;.jar  Sometimes you\u0026rsquo;ll need to specifiy the memory required like so:\n# Specify 4gb as the maximum heap size java -Xmx4g -jar \u0026lt;my_program\u0026gt;.jar  Conda packages Conda is a package manager written in Python and is available for use on our clusters. Package managers make installing easy by automating the process and handing dependencies. Many bioinformatics tools have been wrapped for Conda installation and are in the bioconda channel.\nYou\u0026rsquo;ll need to create your own virtual environment to use Conda, since you won\u0026rsquo;t have write permission to install software in the default location. A virtual environment is an environment where you have your own isolated copy of Conda and your own software packages that you installed via Conda. Virtual environments work by appending the virtual environment\u0026rsquo;s bin directory to your $PATH.\nYou can learn more about Conda here.\nIf you\u0026rsquo;re using the Melbourne Bioinformatics clusters, you\u0026rsquo;ll need to module load Python into your environment path first.\n# If you're using barcoo module load python-intel/3.6  Let\u0026rsquo;s create a new Conda virtual environment and install a package.\n# Create a virtual environment called my_new_env conda create -n my_new_env # Activate the enviroment source activate my_new_env # Your command line prompt should now start with (my_new_env) $ # Look at your path and see the first bin directory is your # conda environment in your home directory echo $PATH  You can manage packages using the conda command.\n# Print conda help conda -h # List conda packages conda list  You can install packages with conda install. For example, bioawk is available in the bioconda channel and you can install it like so:\n# Install bioawk from the bioconda channel conda install -c bioconda bioawk  When you\u0026rsquo;re finished using your software in conda, you can exit the virtual environment with:\nsource deactivate  And when you need your virtual environment again, you can reactivate it with:\nsource activate \u0026lt;my-environment-name\u0026gt;  Compiling from source Compiling from source is another way to install software. Sometimes, only the source code is provided and in this case, you\u0026rsquo;ll need to compile the software yourself.\nA common case is that the source code of the program you want to install is available on GitHub. Often, there are installation instructions in the README or INSTALL file which you can follow.\nIn the most simple case, a make command will run a Makefile and compile the software into a binary file which you can execute. Let\u0026rsquo;s compile bwa as an example.\n# Clone the repository git clone https://github.com/lh3/bwa.git # Change directory cd bwa # Compile make  A new file called bwa should be in the directory which you can run with ./bwa.\nSoftware may also specify it needs to be built with ./configure, make, then make install. The configure script checks the environment the software is to be built in and checks dependencies. Typically, you can also specify additional options when running ./configure such as --prefix=/home/my_username/bin to specify where to install the software. Running the configure script should output a Makefile. Running make will built the software and make install will copy the executable files into the default or specified directory.\nYou can read more about using configure, make, and make install here and here.\n"
},
{
	"uri": "/pearg_documentation/best-practices/backing-up-your-data/",
	"title": "Backing Up Your Data",
	"tags": [],
	"description": "Best practices for backing up your data.",
	"content": " If you only have a single copy of your data, you\u0026rsquo;re vulnerable to losing it. A hardware failure or theft can lead to losing hundreds of hours of work or irreplacable photos.\n\u0026ldquo;Don\u0026rsquo;t worry\u0026mdash; I have a Time Machine backup,\u0026rdquo; I hear you say. Having a Time Machine backup is a good first step, however, you\u0026rsquo;re still at risk in cases where you lose both copies in a house fire or if both your laptop and Time Machine hard drive are stolen.\nHard disk drives are also prone to failure.\n\nOne solution is to store a copy of your data remotely in the cloud. Storage services will also have redundancy measures in place to avoid data loss when hard drives inevitably fail.\nA good rule to follow is the 3-2-1 rule for backing up your data.\nThe 3-2-1 rule for backups The 3-2-1 rule for backup goes as follows:\n Have at least three copies of your data Use at least two different types of media for your copies At least one of your copies should be offsite    Backing up your experimental data Following the 3-2-1 rule, your sequencing data for your study should have at least three copies. A typical case would be:\n one copy on a external hard drive stored in the lab one copy on the volume attached to the server where you\u0026rsquo;re doing your analysis one copy in Mediaflux labeled with appropriate metadata  Cloud-based automated backup services Having a remote backup for your computer is also a good idea. Services such as Backblaze can automatically backup your computer, but they also have a monthly subscription fee at around $5 a month.\nAn example of using the 3-2-1 rule for backing up your laptop would be:\n one copy on the SSD in your laptop one copy on a HDD with Time Machine backups one copy in the cloud using a cloud backup service such as Backblaze  "
},
{
	"uri": "/pearg_documentation/getting-started/getting-help/",
	"title": "Getting Help",
	"tags": [],
	"description": "How to get technical help.",
	"content": "Getting help.\n"
},
{
	"uri": "/pearg_documentation/best-practices/",
	"title": "Best Practices",
	"tags": [],
	"description": "",
	"content": "  Directory Structure  How to structure your project directory when performing an analysis.\n Naming Files  Best practices for naming files.\n Backing Up Your Data  Best practices for backing up your data.\n "
},
{
	"uri": "/pearg_documentation/metadata-documentation/",
	"title": "Metadata Documentation",
	"tags": [],
	"description": "",
	"content": "  Metadata Overview  A overview of our metadata system in use.\n Using Mediaflux  How to use Mediaflux.\n "
},
{
	"uri": "/pearg_documentation/usage-documentation/",
	"title": "Usage Documentation",
	"tags": [],
	"description": "",
	"content": "  Running Jobs with SLURM  Documentation on how to run SLURM jobs.\n Stacks Databases  Best practises for working with Stacks databases.\n Reference Data  Organisation of shared reference data.\n Installing Software  How to install software on our servers.\n "
},
{
	"uri": "/pearg_documentation/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "PEARG Documentation\n"
},
{
	"uri": "/pearg_documentation/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/pearg_documentation/",
	"title": "PEARG Documentation",
	"tags": [],
	"description": "Bioinformatics documentation for PEARG lab members",
	"content": " PEARG Documentation This site contains bioinformatics-related documentation for PEARG lab members.\nGetting started These documents go over what compute resources are available to you, how to get an account on our clusters, and some terms of conditions.\n HPC resources overview Getting an account Tips for Windows users  Best practices These documents describe some best practices when using our compute resources. Not following some of these guidelines may result in a scolding.\n Directory structure Naming files Backing up your data  Metadata Documentation These documents describe how to use our metadata system to record information relating to your samples.\n Metadata overview Using Mediaflux  Usage Documentation Documentation related to using our clusters.\n Running jobs with SLURM  Stacks databases Reference data Installing software  Contribute to this documentation This documentation\u0026rsquo;s source code is hosted on GitHub. Feel free to update the content\u0026mdash; just click the Edit this page link displayed on top right of each page, and create a pull request when done.\nContact Contact me at jchung@unimelb.edu.au or submit an issue on GitHub.\n"
},
{
	"uri": "/pearg_documentation/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]