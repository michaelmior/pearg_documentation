[
{
	"uri": "https://pearg.github.io/pearg_documentation/tutorials/intro-rad-seq/",
	"title": "Introduction to RAD-Seq",
	"tags": [],
	"description": "Introduction to Reference-based ddRAD-Seq using Stacks.",
	"content": "  .spoiler2{ color: black; background-color:black; } .spoiler2:hover { background-color:white; }  Note: Currently, this tutorial is written to be performed on the mozzie server. In the future, this will be expanded so it can be done on any linux server.\nThis tutorial goes through the steps of setting up a project directory, demultiplexing RAD-Seq data, aligning RAD-seq samples to a reference genome, building the loci catalogue and calling SNPs with Stacks, and generating a PCA plot of the SNP data.\nPre-requisites Before beginning this tutorial, you should be familiar with RAD-seq for SNP discovery. If you\u0026rsquo;re not familiar with RAD-seq, we recommend you read this review paper to help you get started.\nThis tutorial also assumes you\u0026rsquo;re familiar with Unix and running commands on the command line interface. If you\u0026rsquo;re using a shared cluster, you should also be familiar with running jobs through SLURM.\nThe last section also assumes some basic knowledge of R.\nLearning objectives  Understand the nature of RAD-seq sequencing data Process ddRAD data using the Stacks reference-based workflow Be able to calculate some statistics and generate a PCA plot using the SNP results  The data The data this tutorial uses is from Aedes Aegypti mosquitos collected from Indonesia and Malaysia. The samples were prepared following a protocol adapted from the methods described here. The restriction enzymes used for the double digest were nlaIII and mluCI.\nTo save time, we\u0026rsquo;ll be using a subset of the full data in this tutorial. Only reads belonging to Aedes Aegypti Chr1:1-1000000 have been included in the sequencing data.\nThe size of the tutorial data is around 500 MB. In real situations, you would expect libraries to be in the order of 10s of gigabytes, and have multiple libraries in a project.\nSetup the project directory Let\u0026rsquo;s begin by setting up a directory for your project. Good project organisation will make your life easier in the long run, so spend some time at the outset to do it properly. A few months after your initial analysis, you may not remember what you did or what your files contain. Detailed documentation and a sensible directory structure can help with this.\nGo to the directory where you wish to store this project, and create a new directory for this tutorial. An example project name could be rad_seq_tutorial.\ncd /path/to/sensible/project/directory mkdir rad_seq_tutorial cd rad_seq_tutorial # Store the tutorial path in a variable so we can refer to it later TUTORIAL_DIR=$(pwd)  In this directory, we\u0026rsquo;ll create some new directories where we\u0026rsquo;ll store our data and results.\nmkdir raw_data demuxed_seq results scripts software  We\u0026rsquo;ll also create a few sub-directories in results where we\u0026rsquo;ll store output from different programs that we\u0026rsquo;ll run. These will be stored in results_2020-02-01 (change the date to your current date). When performing bioinformatics, it\u0026rsquo;s not unusual to have to re-run analyses again sometime down the track (e.g. the addition of a new sequencing library, changing of parameters for tools, fixing mistakes). Including the date in the directory name is an easy way to identify different sets of analyses in the future.\ncd results mkdir results_2020-02-01 cd results_2020-02-01 mkdir alignments gstacks populations  Lastly, create a README.txt text file with a brief description of what the project directory is about using your favourite text editor. For example:\n# RAD-seq Tutorial This directory contains analysis of RAD-seq data from Indonesia and Malaysia from the Intro to RAD-Seq tutorial.  Your project directory should look something like this:\nrad_seq_tutorial/ ├── demuxed_seq/ ├── raw_data/ ├── results/ │ ├── results_2020-02-01/ │ │ ├── alignments/ │ │ ├── gstacks/ │ │ └── populations/ ├── scripts/ ├── software/ └── README.txt  Get the data If you\u0026rsquo;re using the mozzie server, copy the files in /mnt/galaxy/shared/tutorial_data/rad_seq_workshop/ into the raw_data directory of your tutorial project directory.\ncd $TUTORIAL_DIR cd raw_data cp -r /mnt/galaxy/shared/tutorial_data/rad_seq_workshop/* .  Note that our library data is stored in another sub-directory, tute_library. It\u0026rsquo;s common to have multiple libraries for a single project, therefore a good idea to have a separate sub-directories for each of your libraries.\nIn the library directory, the paired end sequencing data is in the two FASTQ files:\n tute_library_R1.fastq.gz corresponds to the forward reads from a PE library tute_library_R2.fastq.gz corresponds to the reverse reads from a PE library  Also included is:\n barcodes.txt: the barcode file that will be used to separate individual samples from the FASTQ file  There are also two files called popmap_01.txt and popmap_02.txt that we\u0026rsquo;ll discuss later.\nYour raw data directory should now look like this:\nraw_data/ ├── tute_library/ │ ├── barcodes.txt │ ├── tute_library_R1.fastq.gz │ └── tute_library_R2.fastq.gz ├── popmap_01.txt └── popmap_02.txt  Demultiplexing Examine the data Multiple samples are sequenced in a single RAD-seq library. Each sample has a unique combination of barcodes that we can use to separate which reads belong to each sample. The program that we\u0026rsquo;ll use, process_radtags, requires this information to be in a specific form.\nNavigate to the raw_data/tute_library directory. Take a look at the barcodes.txt file to see an example of a barcode file.\ncat barcodes.txt  CTAGTC\tGTACTG\tindonesia_01 AGCTGA\tGTACTG\tindonesia_02 TCGACT\tGTACTG\tindonesia_03 ATGCTA\tGTACTG\tindonesia_04 GCATAG\tGTACTG\tindonesia_05 TGCACA\tCATGAC\tmalaysia_01 CATGTG\tCATGAC\tmalaysia_02 GTACAC\tTGCAGT\tmalaysia_03 ACGTGT\tTGCAGT\tmalaysia_04 AGCTCT\tTGCAGT\tmalaysia_05 GATCTC\tTGCAGT\tunknown_01 AGCTGA\tACGTCA\tunknown_02  The barcode file is a tab-separated file where the first column corresponds to the p1 barcode (the barcode that will be found at the beginning of the read), the second column corresponds to the p2 barcode (the barcode that will be found a the end of the read), and the third column represents the sample name.\nFrom this file, we can see there are 12 samples we\u0026rsquo;d like to demultiplex from the library: five samples from Indonesia, five from Malaysia, and two samples that have not been labelled with their location of origin.\nLet\u0026rsquo;s now have a look at the raw FASTQ data. If you\u0026rsquo;re unfamiliar with the FASTQ format, you can read up on it here.\nUse less to view the R1 reads (i.e. the forward reads of this library):\nless tute_library_R1.fastq.gz # press 'q' to quit when you're done  Look at the first read of the file which makes up the first four lines:\n@SN7001291:345:HFKWVBCXX:2:1101:1829:1987 1:N:0:2 ATCGCTCATGCCAGCTCAAAATAATCAACTTCGAGGTGCACCAATCCTAGCGCTATCAGACCTCCCTCGCTATGAGACGGCTGCGCATTTTCATTCCCTTG + GGGGGIIIIIIIIIIIIIIIIIIIIIIIIIIIIGIIIIIIIIIIIIIIIGGGGIIIIIIIGIIIIIIIIIIIIIIIIIIIIIGIGIGGGIIIIIIIIIGGG   The first line contains the read identifier. Each identifier begins with an @ character and is unique. The second line contains the nucleotide sequence of the read. The third line is a separator line and contains a + character. The fourth line contains the PHRED quality score (i.e. a measurement of base confidence) for each of the bases in the nucleotide sequence.  Let\u0026rsquo;s look at the second line (i.e. the sequence) more closely:\nATCGCTCATGCCAGCTCAAAATAATCAACTTCGAGGTGCACCAATCCTAGCGCTATCAGACCTCCCTCGCTATGAGACGGCTGCGCATTTTCATTCCCTTG  The first six bases (e.g. ATCGCT) of each read in this file correspond to the p1 barcode sequence. The next four bases are CATG which corresponds to the restriction site where nlaIII cleaves.\n5'- C A T G | -3' 3'- | G T A C -5'  All reads in the R1 file should have this restriction site sequence immediately following the barcode sequence.\nNow, look at the R2 file (i.e. the reverse reads):\nless tute_library_R2.fastq.gz # press 'q' to quit when you're done  The first read in this file is\n@SN7001291:345:HFKWVBCXX:2:1101:1829:1987 2:N:0:2 CATGACAATTACCCTTTGTCAAGAAGGCGAAGATGTCTCCCTTTAGGTGGTAAGTCAACTTACCAGAAAGAAGTTGAGCAATGTGGAAAAATCCACCATTC + GGIGGIGGIGIGIGGIIIIIIIIIIGIIGIGIIIGIIGGGIIGGGGGAGGGIIIIIIIIIIGIIIIIIIIIGIIIIIIIIIIIGIIIIIIIIIIIIGGGGG  Like the R1 file, the first six bases (CATGAC) correspond to the p2 barcode sequence. Since the first read of the R1 file and the first read of the R2 file are mate pairs (i.e. they were sequenced from the same DNA fragment), we can use both the p1 and p2 barcodes to determine what sample the sequence belongs to.\nThe next four bases are AATT which corresponds to the restriction site were mluCI cleaves.\n5'- | A A T T -3' 3'- T T A A | -5'  These two reads are both sequenced from the same DNA fragment.\nTODO: Insert image here Question:\nFor a specific set of paired-end reads, the p1 barcode is observed to be ATGCTA and the p2 barcode is observed to be GTACTG. What sample does the read belong to?\nAnswer:\nindonesia_04\nRun process_radtags The Stacks suite has a program called process_radtags that demultiplexes RAD-seq data. We\u0026rsquo;ll run it on our tutorial data and provide it the barcode file like so:\ncd $TUTORIAL_DIR process_radtags \\ -1 raw_data/tute_library/tute_library_R1.fastq.gz \\ -2 raw_data/tute_library/tute_library_R2.fastq.gz \\ -o demuxed_seq \\ -b raw_data/tute_library/barcodes.txt \\ --renz_1 nlaIII \\ --renz_2 mluCI \\ --paired \\ -i gzfastq \\ --inline_inline \\ -c -q -r -t 80 -s 20  Generally, if you\u0026rsquo;re running commands on a shared server, you should be using SLURM to submit your jobs. However, this tutorial data only takes a few seconds to process, so if you\u0026rsquo;re using the mozzie server, you can just run it on the master node without SLURM. With real data, jobs can take hours to run, so make sure you don\u0026rsquo;t run large jobs on the master node.\n process_radtags produces four files for each sample. For example, for indonesia_01:\n indonesia_01.1.fq.gz: contains R1 (forward) reads belonging to the sample indonesia_01.2.fq.gz: contains R2 (reverse) reads belonging to the sample indonesia_01.rem.1.fq.gz: contains orphaned R1 reads that don\u0026rsquo;t have a matching R2 read indonesia_01.rem.2.fq.gz: contains orphaned R2 reads that don\u0026rsquo;t have a matching R1 read  Reads can be orphaned when the mate pair\u0026rsquo;s read restriction enzyme site doesn\u0026rsquo;t match or the read is dropped due to low quality.\nQuestion:\nHave a look at the log file in process_radtags. How many reads matched the barcode for the indonesia_01 sample? After filtering those without a matching restriction site and low quality, how many reads have been retained for indonesia_01?\nAnswer:\n258052 total sequences matched indonesia_01. 229479 were retained after removing reads with restriction site not matching (23282) and of low quality (5291).\nQuestion:\nIn the process_radtags command you ran, what does the option -t 80 and -s 20 do? (Hint: check the manual)\nAnswer:\nThe \u0026ldquo;-t 80\u0026rdquo; option truncates the read to a final length of 80 bp. The \u0026ldquo;-s 20\u0026rdquo; option tells the program to discard the read due to low quality if the average quality score drops below 20 in a sliding window (whose size is set by the \u0026ldquo;-w\u0026rdquo; option and has a default of 0.15).\nAlignment The reference genome Next we\u0026rsquo;ll align the demultiplexed reads to the reference Aedes Aegypti reference genome using bowtie2. First, have a look at the reference assembly FASTA file. On the mozzie server:\ncd /mnt/galaxy/shared/genomes/GCF_002204515.2_AaegL5.0 less GCF_002204515.2_AaegL5.0_genomic.fna # press 'q' to quit when you're done  This large file contains the 1.9 gigabases that make up the Aedes Aegypti genome. Most of the bases have been assembled into three chromosomes: NC_035107.1 (chr1), NC_035108.1 (chr2), and NC_035109.1 (chr3). Since the assembly is imperfect, there are also thousands of smaller scaffolds that have yet to be placed to a chromosome.\nIf you\u0026rsquo;re working on the mozzie server, the genome has already been indexed and is ready to be used for alignment with bowtie2. If you\u0026rsquo;re working on another server, you\u0026rsquo;ll need to download the genomic FASTA file for GenBank assembly GCF_002204515.2 yourself. You\u0026rsquo;ll also need to build a Bowtie index for the reference genome using bowtie2-build.\nBowtie2 Alignment is the process of mapping reads from your FASTQ files to a location on the reference genome. Bowtie2 is a commonly used aligner that can quickly align short reads to a reference. We\u0026rsquo;ll now align the demultiplexed reads to the Aegypti reference genome we viewed earlier. For indonesia_01:\ncd $TUTORIAL_DIR/results/results_2020-02-01 bowtie2 \\ -p 2 \\ -x /mnt/galaxy/shared/genomes/GCF_002204515.2_AaegL5.0/GCF_002204515.2_AaegL5.0_genomic \\ --very-sensitive \\ -1 ../../demuxed_seq/indonesia_01.1.fq.gz \\ -2 ../../demuxed_seq/indonesia_01.2.fq.gz \\ 2\u0026gt; alignments/indonesia_01.bt2.log \\ | samtools view -bS \\ | samtools sort -o alignments/indonesia_01.bam  We\u0026rsquo;ve chosen the --very-sensitive preset option which is through in trying to align reads to the reference genome, but takes a longer time than other presets. The SAM output from bowtie2 is piped into samtools to convert it to BAM, and then piped again to sort the BAM file. If you\u0026rsquo;re unfamiliar with the SAM/BAM format, you can read up on it here.\nTake a look at the BAM file:\nsamtools view alignments/indonesia_01.bam | less # press q to quit when you're done  If you want to have a more in-depth look at the options that bowtie2 has, you can view the help message with the -h flag:\nbowtie2 -h  or have a look at the bowtie2 manual.\nWe can also write a loop to process all samples sequentially. For example, the code below uses the barcode file\u0026rsquo;s to extract the sample names in the library, and then runs bowtie2 on the sample\u0026rsquo;s demultiplexed FASTQ file.\nwhile read line; do sample=$(echo $line | cut -d \u0026quot; \u0026quot; -f 3) echo Aligning sample: $sample bowtie2 \\ -p 2 \\ -x /mnt/galaxy/shared/genomes/GCF_002204515.2_AaegL5.0/GCF_002204515.2_AaegL5.0_genomic \\ --very-sensitive \\ -1 ../../demuxed_seq/${sample}.1.fq.gz \\ -2 ../../demuxed_seq/${sample}.2.fq.gz \\ 2\u0026gt; alignments/${sample}.bt2.log \\ | samtools view -bS \\ | samtools sort -o alignments/${sample}.bam done \u0026lt; ../../raw_data/tute_library/barcodes.txt  Question:\nTake a look at the log file malaysia_02.bt2.log. What is the overall alignment rate for the sample? Given what you know about this tutorial data, why might this alignment rate be higher than normal?\nAnswer:\nThe observed overall alignment rate of this sample is 93.84%. This alignment rate is not representative of a real RAD-seq experiment because the tutorial data is made up of reads that were selected because they were known to align to a defined part of this genome. If we were working with the full set of data, the alignment rate would be reduced.\nStacks Stacks is a suite of software tools that can processes RAD-seq data, call SNPs, and calculate some popgen statistics. In reference-based RAD-seq analysis, we\u0026rsquo;ll be using the programs gstacks and populations.\nRun gstacks The next step is to build a catalogue of RAD-tags using gstacks. You can read more about the program and its options here.\nThere are two valid ways you can define which BAM files you input into gstacks: using a popmap file or listing each bam file in the command. We\u0026rsquo;ll be using the latter. Here we\u0026rsquo;ll use the -M argument to give gstacks a popmap file and the -I argument to give it our alignments directory.\nThe popmap file is a simple tab-delimited file that defines what population each sample is from. The populations program that we\u0026rsquo;ll run immediately after gstacks can use this information to calculate common population genetics statistics such as FST and nucleotide diversity.\nIn some cases, you won\u0026rsquo;t be interested in the statistics calculated by populations (since you\u0026rsquo;ll be doing your own analyses downstream). If this is the case, you can just define all samples as belonging in the same group. Have a look at the popmap file included in the raw_data directory:\ncd $TUTORIAL_DIR/raw_data/ cat popmap_01.txt  In this directory, there\u0026rsquo;s also another popmap file called popmap_02.txt. You can use this popmap file if you want to define groups in downstream Stacks analysis.\nQuestion:\nHave a look at the popmap_02.txt file. How many unique populations are there?\nAnswer:\nThere are three unique populations in the file: \u0026ldquo;indonesia\u0026rdquo;, \u0026ldquo;malaysia\u0026rdquo;, and \u0026ldquo;unknown\u0026rdquo;.\nNow go back to the results directory and run gstacks to create the catalogue. We\u0026rsquo;ll use the popmap_01.txt file here, but note that gstacks doesn\u0026rsquo;t use the population information, only populations does.\ncd $TUTORIAL_DIR/results/results_2020-02-01 gstacks \\ -I alignments \\ -M ../../raw_data/popmap_01.txt \\ -O gstacks \\ --min-mapq 20 \\ -t 4  Have a look at the catalog.fa.gz output from gstacks using less.\ncatalog.fa.gz is a FASTA file that contains the RAD tags that have been detected in the given RAD-seq samples. The header of each sequence contains the position of where the RAD-tag is in the reference genome (e.g. pos=CM008043.1:9352:-) and the number of samples the RAD-tag was found (e.g. NS=1).\ncatalog.calls is a binary file that contains the genotyping data that populations will use for calling SNPs.\nRun populations Populations can do a number of calculations and statistical tests, but for this tutorial, we\u0026rsquo;ll only be using it to output a VCF file of our SNP calls. You can read up on populations here.\nWhen we run populations, we\u0026rsquo;ll be writing to a sub-directory in the populations results directory. It can be useful to have multiple directories for populations as re-running it with different parameters, samples, etc. is common.\nRun populations requiring no missing data for any SNPs:\nmkdir populations/pop_01 populations \\ -P gstacks \\ -M ../../raw_data/popmap_01.txt \\ -O populations/pop_01 \\ -t 1 \\ -p 1 \\ -r 1 \\ --write_single_snp \\ --vcf  Have a look at the VCF file. If you\u0026rsquo;re not familiar with the VCF format, you can read up on it here.\nQuestion:\nApproximately many SNPs are there in the VCF file?\nAnswer:\nThere are 987 SNPs in the VCF file.\nQuestion:\nFor the first SNP in the VCF file (chromosome NC_035107.1, position 119501), what is the genotype call for indonesia_01? What nucleotide bases would be observed?\nAnswer:\nFor indonesia_01, the GT field is \u0026ldquo;0/1\u0026rdquo; which means the sample is heterozygous at that position. Since the REF base is C and the ALT base is A, the genotype would be C/A.\nRe-run populations in a new directory (pop_02) using less stringent parameters. For this example, we\u0026rsquo;ll use the popmap_02.txt file to define populations and require SNPs to be called in at least 80% of individuals (-r 0.8) in at least two of the three populations (-p 2). We\u0026rsquo;ll also include the --fstats flag to enable SNP-based F-statistics.\nmkdir populations/pop_02 populations \\ -P gstacks \\ -M ../../raw_data/popmap_02.txt \\ -O populations/pop_02 \\ -t 1 \\ -p 2 \\ -r 0.8 \\ --write_single_snp \\ --vcf \\ --fstats  Since we defined populations and use the --fstats option, we now are given pairwise estimates of FST in our output. Go to the output directory and view some of these files. The files beginning with populations.fst_* contain the SNP-level measures of FST between two populations and populations.fst_summary.tsv contains the pairwise FSTs for the populations.\nAdditionally, with our less stringent parameters, the new VCF file now has more than twice the number of SNPs included, but some samples have missing genotype calls (i.e. ./.).\nQuestion:\nWhat does the --write_single_snp option do in populations? Why might you want to use this option when you run populations? (Hint: look at the help page or manual)\nAnswer:\nThe \u0026ldquo;--write_single_snp\u0026rdquo; restricts analysis to only use the first SNP on the RAD tag. Often you want your SNPs to be independent observations (i.e. not under linkage disequilibrium). Using the \u0026ldquo;--write_single_snp\u0026rdquo; or \u0026ldquo;--write-random-snp\u0026rdquo; flag removes SNPs which are located on the same RAD tag, and therefore very close together in the genome.\nDownstream analysis Generally, the VCF files from populations will form the basis of your downstream analysis of population genetics.\nHere, we\u0026rsquo;ll go through a toy example of using the SNP data to produce a PCA plot and identifying which populations the two unknown samples belong to, but for serious popgen analysis, you\u0026rsquo;ll need to turn to software packages like adegenet, vegan, and pegas.\nWe\u0026rsquo;ll first convert the VCF file into a \u0026ldquo;012\u0026rdquo; matrix using VCFtools. This format represents homozygous REF calls as \u0026ldquo;0\u0026rdquo;, heterozygous calls as \u0026ldquo;1\u0026rdquo; and homozygous ALT calls as \u0026ldquo;2\u0026rdquo;.\n# Go to the pop_01 directory of populations cd $TUTORIAL_DIR/results/results_2020-02-01/populations/pop_01 vcftools \\ --vcf populations.snps.vcf \\ --012 \\ --out populations.snps  Have a look at the output files produced by VCFtools.\nThe next step is to load the files into R. If you\u0026rsquo;re on the mozzie server, you can just use the RStudio server provided. If you\u0026rsquo;re not using the mozzie server, download the populations.snps.012 and populations.snps.012.indv files to your local computer.\nIn R, this code generates a simple PCA plot:\n# Load 012 files (change the paths) gt_matrix \u0026lt;- read.table(file=\u0026quot;/path/to/rad_seq_tutorial/results/results_2020-02-01/populations/pop_01/populations.snps.012\u0026quot;, row.names = 1, header=FALSE) sample_names \u0026lt;- read.table(file=\u0026quot;/path/to/rad_seq_tutorial/results/results_2020-02-01/populations/pop_01/populations.snps.012.indv\u0026quot;, header=FALSE) # View the first bit of the genotype matrix gt_matrix[1:5,1:5] # Create a dataframe containing sample info sample_info \u0026lt;- data.frame( sample_id=sample_names[,1], group=str_remove(sample_names[,1], \u0026quot;_\\\\d\\\\d$\u0026quot;), stringsAsFactors=FALSE) sample_info # PCA plot pca \u0026lt;- prcomp(gt_matrix)$x[,1:4] pca \u0026lt;- cbind(pca, sample_info) ggplot(pca, aes(x=PC1, y=PC2, color=group)) + geom_point()  TODO: Insert image here Question:\nThere are two samples from unknown origin. After looking at the PCA plot, what country do you think they originate from?\nAnswer:\nThe two unknown samples both cluster more closely to the Malaysian samples, so it is likely they are from Malaysia.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/metadata-documentation/metadata-overview/",
	"title": "Metadata Overview",
	"tags": [],
	"description": "A overview of our metadata system in use.",
	"content": "The Hoffmann Lab has a system for storing its raw sequencing data. It is important you follow the system in place and ensure all your data is properly labelled and backed up. This is to ensure that sequencing data generated at the present time can still be used in future analyses, even if you are no longer at the lab. Storing our next-generation sequencing data in a systematic manner also benefits the whole lab as other lab members can find and use data that could be beneficial to their project. Sharing is caring.\nThe Mediaflux is the service we use to manage our data, with the underlying storage being hosted by VicNode. With Mediaflux, we can upload sequencing libraries into storage located in the cloud, then set comprehenseive metadata detailing what was sequenced, where the biological samples are stored, and other important experimental design information.\nTo get started using Mediaflux, visit the Using Mediaflux page.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/usage-documentation/running-jobs-with-slurm/",
	"title": "Running Jobs with SLURM",
	"tags": [],
	"description": "Documentation on how to run SLURM jobs.",
	"content": " Using a computer cluster with other users means sharing resources. SLURM (Simple Linux Utility for Resource Management) is a commonly used job scheduler that manages a queue where you submit your jobs and allocates resources to run your job when resources are available.\nThe documentation on using SLURM at Melbourne Bioinformatics is quite comprehensive and can be found here.\nChecking the status of your jobs squeue You can view information about jobs in the SLURM queue with the squeue command. View the help message with squeue --usage or the manual with man squeue.\n# List all jobs in the queue squeue # List all jobs for account UOM0041 squeue --account=UOM0041 # List all jobs in the queue for user jchung in long format squeue -l -u jchung  If you\u0026rsquo;re using snowy or barcoo, you can also use the showq command.\nscontrol show job If you want information regarding a specific job id, you can use scontrol.\nscontrol show job \u0026lt;job-id\u0026gt;  sacct You can check the status of recently finished jobs with sacct.\nsacct  sinfo You can also view the status of the nodes in the cluster with sinfo.\n# Show status of all nodes sinfo -Nel  If you want more information about a specific node, you can use scontrol.\n# View information on the master node scontrol show node master  Running your jobs sbatch Most of your jobs will be sumbitted to SLURM via sbatch. The commands that you want to run need to be written in a script (a plain-text file that we\u0026rsquo;ll discuss further below), saved to a location, then submitted using sbatch.\n# Print the help message from sbatch sbatch --help # Submit your script by specifying the name of your script sbatch my-script.sh  sinteractive You can use the sinteractive command to run your job in an interactive session. When SLURM allocates your job resources, you will be provided with an interactive terminal session. It is recommended to use sinteractive in conjunction with a terminal multiplexer such as GUN Screen so the job won\u0026rsquo;t terminate if you disconnect from the server.\n# Print the help message for sinteractive sinteractive --help # Submit a job with the default parameters sinteractive # Submit a job with 4 CPUs, 16 GB memory, and wall time of 1 day sinteractive --ntasks=1 --cpu-per-task=4 --mem=16384 --time=1-0:0:0  Note that the memory amount is specified in MB.\nscancel You can cancel a running job or a job in the queue with scancel.\nscancel \u0026lt;job-id\u0026gt;  Writing a SLURM script For beginners, I recommend using the job script generator written by Melbourne Bioinformatics. If you\u0026rsquo;re using one of the PEARG clusters on the Nectar cloud (i.e. mozzie or rescue), you can ignore the \u0026ldquo;Project ID\u0026rdquo; and the \u0026ldquo;Modules\u0026rdquo; field.\nHere\u0026rsquo;s an example of a simple SLURM script running on the mozzie server.\n#!/bin/bash #SBATCH --job-name=denovo_map #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 denovo_map.pl \\ -m 3 -M 2 -n 1 -T 8 -b 1 -S \\ -o denovo_map_m3_M2_n1_2017-11-09 \\ -s ../processed_radtags/sample-1.fastq.gz \\ -s ../processed_radtags/sample-2.fastq.gz \\ -X \u0026quot;populations: --vcf\u0026quot;  The first line of the script must specify the interpreter the script will be executed with such as bash or sh. Keep this as #!/bin/bash unless you have reason to change it.\nEach line starting with #SBATCH is an option that SLURM\u0026rsquo;s sbatch command uses. You can get view all available options with sbatch -h or by viewing the man page with man sbatch.\nThe most common #SBATCH options you\u0026rsquo;ll most likely be using are:\n --job-name=XXX: You should always specify a job name for your job --nodes=1: In most cases, you should be requesting one node so all the requested CPUs are on the same node. --ntasks=1: In most cases you\u0026rsquo;ll be running one task per job --cpus-per-task=X: Specify the number of CPUs to request.  You can also direct your stdout and stderr into defined files:\n -output my-file-%j.out -error my-file-%j.err  If you\u0026rsquo;re using barcoo or snowy, you\u0026rsquo;ll also need to specify memory in MB with:\n --mem=XXXXX for jobs using multiple CPUs, or --mem-per-cpu=XXXX for single CPU jobs.  With barcoo and snowy, you\u0026rsquo;ll also need to specify the partition, and a time limit:\n -p main: The partition is called \u0026lsquo;main\u0026rsquo; --time=D-HH:MM:SS: Time limit given for the job. If the job exceeds the time, it is automatically terminated.  Here\u0026rsquo;s an example of a SLURM script for barcoo.\n#!/bin/bash # Partition for the job: #SBATCH -p main # Account to run the job: #SBATCH --account=VR0002 # Multithreaded (SMP) job: must run on one node #SBATCH --nodes=1 # The name of the job: #SBATCH --job-name=\u0026quot;test-job\u0026quot; # Maximum number of tasks/CPU cores used by the job: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 # The amount of memory in megabytes per process in the job: #SBATCH --mem=32768 # The maximum running time of the job in days-hours:mins:sec #SBATCH --time=0-1:0:00 # Run the job from your home directory: cd $HOME # The job command(s): sleep 10  When using barcoo and snowy, don\u0026rsquo;t forget to module load the software you need into your environment path.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/best-practices/directory-structure/",
	"title": "Directory Structure",
	"tags": [],
	"description": "How to structure your project directory when performing an analysis.",
	"content": " When using the PEARG clusters or the Melbourne Bioinformatics clusters, your files and data should be organised according to the convention described here.\nEach project should have its own directory located inside your home directory or a shared location. Your project directory name should be meaningful.\nAny shared data such as reference genomes and indices should be stored outside your project directory to avoid unnecessary duplication of data.\nCheck your project directory At absolute minimum, please check:\n is your project name sensible? do you have a README file (or equivalent) in the parent directory of your project directory that contains meaningful information? do you have a data directory (containing your raw data) and a results directory (containing processed data) with files organised in a logical manner?  If you\u0026rsquo;ve answered no to any of these questions, you may get a stern email in the future.\n  The motivation behind having a strict structure that all members adhere to for project organisation is transparency. Anyone from the lab should be able to look in your project directory and clearly understand what was run and reproduce your analysis. Most of the time, the person trying to decode what analyses were performed and why will be future you. Cooperate with your future self by leaving verbose notes in README files!\nWhile the sysadmin won\u0026rsquo;t be authoritarian about the precise directory structure, any flagrant disregard for the guidelines (such as dumping all your processing files in the top-level results directory) will be met with consequences.\nFilesystem overview Click on the image to view a larger version of the recommended directory structure or scroll down for another example in text.\n\nCreating a project directory using the template # TODO: Jess will create a project skeleton in the future  You should rename the directories starting with rename_* into something sensible.\nExample directory structure In this example, our project name is called project_name and is a RAD-seq experiment that was processed using Stacks.\nproject_name/ ├── data/ ├── results/ ├── scripts/ ├── software/ └── README  The directory has four subdirectories: data, results, scripts, and software, and one README file. The README file should be a plain-text file containing basic project information (e.g. what the project is about, what type of data was sequenced).\nData directory The data directory should contain the raw data received from sequencing. Each library should have it\u0026rsquo;s own directory containing sequencing files and a text file containing barcodes corresponding to samples. This file is needed for Stacks process_radtags.\nproject_name/ ├── data/ │ ├── library_1_raw_data/ │ │ ├── seqA_R1_001.fastq.gz │ │ ├── seqA_R2_001.fastq.gz │ │ └── library_1_barcodes.txt │ ├── library_2_raw_data/ │ │ ├── seqB_R1_001.fastq.gz │ │ ├── seqB_R2_001.fastq.gz │ │ └── library_2_barcodes.txt ├── results/ ├── scripts/ ├── software/ └── README  Make files read-only (optional) The files in your data directory should never be edited.\nIf you are familiar with UNIX file permissions, you can remove write permissions with the chmod command. For example, the following command removes write permission for all users:\nchmod a-w seqA_R1_001.fastq.gz  You can check file permissions with ls -l where the first column represents whether read/write/execute access is avaiable.\n$ ls -l -r--r--r-- 1 jess jess 142870 Aug 8 14:30 seqA_R1_001.fastq.gz -r--r--r-- 1 jess jess 177552 Aug 8 14:30 seqA_R2_001.fastq.gz  Results directory The results directory should have one directory for each time you generate a set of results. Using subdirectories inside the main results directory is recommended because often experiments are re-run in the future (e.g. updated software versions, more sequencing data, reanalysis before publication). I recommend naming the directory with a date in YYYY-MM-DD format at the beginning of the name so the directories are sorted chronologically.\nproject_name/ ├── data/ ├── results/ │ ├── 2017-08-01_results/ │ │ ├── ... │ │ ├── ... │ │ └── ... │ ├── 2017-11-01_extra_samples/ │ │ ├── ... │ │ ├── ... │ │ └── ... │ ├── 2018-05-01_reanalysis/ │ │ ├── ... │ │ ├── ... │ │ └── ... ├── scripts/ ├── software/ └── README  Inside each result subdirectory, there should be multiple directories containing output from steps in your workflow. In this example, the directories inside 2017-08-01_results are: demuxed_seq, demuxed_cat, alignments, stacks, and qc. Your directory names may look different depending on what type of analysis you\u0026rsquo;re performing. The contents of each directory is described below.\nSequencing data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ │ ├── mozzie-1.1.fq │ │ ├── mozzie-1.2.fq │ │ ├── mozzie-1.rem.1.fq │ │ ├── mozzie-1.rem.2.fq │ │ ├── mozzie-2.1.fq │ │ ├── mozzie-2.2.fq │ │ └── ... │ ├── demuxed_cat/ │ │ ├── mozzie-1.fq │ │ ├── mozzie-2.fq │ │ ├── mozzie-3.fq │ │ └── ... │ └── ...  The demuxed_seq directory contains demuxed sequencing data processed by process_ragtags. Stacks should output four files for each sample listed in the barcode file. In this example, mozzie-1.1.fq and mozzie-1.2.fq contain the set forward and reverse reads for the mozzie-1 sample. The mozzie-1.rem.1.fq and mozzie-1.rem.2.fq files contain the remaining reads that are unpaired due to their mate being discarded.\nIf you\u0026rsquo;re working with ddRADseq data, Stacks recommends concatenating the four files together. Here, demuxed_cat contains the concatenated files.\nAlignment data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ │ ├── AaegL2/ │ │ │ ├── mozzie-1.AaegL2.sorted.bam │ │ │ ├── mozzie-1.AaegL2.sorted.bam.bai │ │ │ ├── mozzie-2.AaegL2.sorted.bam │ │ │ ├── mozzie-2.AaegL2.sorted.bam.bai │ │ │ ├── ... │ │ │ └── AagL2_alignment_code.txt │ │ ├── AaegL3/ │ │ │ ├── mozzie-1.AaegL3.sorted.bam │ │ │ ├── mozzie-1.AaegL3.sorted.bam.bai │ │ │ ├── mozzie-2.AaegL3.sorted.bam │ │ │ ├── mozzie-2.AaegL3.sorted.bam.bai │ │ │ ├── ... │ │ │ └── AagL3_alignment_code.txt │ └── ...  Alignments should be stored in the alignments directory with a separate directory for each reference genome aligned against. Alignments should be stored as bam files with the .bam suffix and bam index files, if provided, should end with .bai. If alignements are sorted, it\u0026rsquo;s recommended to include sorted in the filename. Including the reference genome name in the filename is also helpful.\nA plain-text file with what commands were run should also be included in the directory (e.g. AagL2_alignment_code.txt) or in the scripts directory.\nStacks data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ ├── stacks/ │ │ ├── stacks_AaegL2_females/ │ │ │ ├── catalog/ │ │ │ │ ├── mozzie-1.AaegL2.alleles.tsv │ │ │ │ ├── mozzie-1.AaegL2.matches.tsv │ │ │ │ ├── mozzie-1.AaegL2.models.tsv │ │ │ │ ├── mozzie-1.AaegL2.snps.tsv │ │ │ │ ├── mozzie-2.AaegL2.alleles.tsv │ │ │ │ ├── ... │ │ │ │ ├── batch_1.catalog.alleles.tsv │ │ │ │ ├── batch_1.catalog.snps.tsv │ │ │ │ ├── batch_1.catalog.tags.tsv │ │ │ │ └── batch_1.markers.tsv │ │ │ ├── population_females_filtered/ │ │ │ │ ├── batch_1.vcf │ │ │ │ ├── batch_1.haplotypes.tsv │ │ │ │ ├── batch_1.sumstats_summary.tsv │ │ │ │ ├── batch_1.sumstats.tsv │ │ │ │ ├── batch_1.hapstats.tsv │ │ │ │ └── code_females_filtered.txt │ │ │ ├── population_females_singlesnp/ │ │ │ │ ├── batch_1.vcf │ │ │ │ ├── batch_1.haplotypes.tsv │ │ │ │ ├── batch_1.sumstats_summary.tsv │ │ │ │ ├── batch_1.sumstats.tsv │ │ │ │ ├── batch_1.hapstats.tsv │ │ │ │ └── code_females_singlesnp.txt │ │ ├── stacks_AaegL2_males/ │ │ │ └── ... │ │ └── ... │ └── ...  Each run of Stacks to get a catalogue should have its own separate directory in the stacks directory. The output files from ref_map or denovo_map stored in its own directory.\nEach time you run Stacks populations with designated filters, you should store the files in a separate directory. You should also include a file containing the code that was used to produce the output.\nQC data results/ ├── 2017-08-01_results/ │ ├── demuxed_seq/ │ ├── demuxed_cat/ │ ├── alignments/ │ ├── stacks/ │ ├── qc/ │ │ ├── fastqc/ │ │ │ ├── ... │ │ │ ├── ... │ │ │ └── ... │ │ ├── flagstat/ │ │ │ ├── ... │ │ │ ├── ... │ │ │ └── ... │ │ └── ... │ └── ...  The qc directory should contain the output of programs run for quality control purposes (e.g. fastQC, samtools flagstat).\nScripts directory It\u0026rsquo;s up to you if you want to store your scripts inside the scripts directory or with the output files that were generated. Just make sure you document all the code that was run somewhere sensible.\nSoftware directory If you have any additional software you compiled specifically for your project, you can store them here.\nREADME files README files are plain-text files where you should write descriptions of what the directory contains, what analysis was done, why certain parameters were chosen, what results were found, etc. Place a README file in any directory you feel could use one. Documenting your work clearly is good practice and often pays dividends in the future.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/getting-started/hpc-resources/",
	"title": "Compute Resources",
	"tags": [],
	"description": "An overview of high-performance computing resources available.",
	"content": " This page lists the compute resources available to PEARG lab members.\nCloud Servers These clusters are hosted on the Nectar cloud. Email jchung@unimelb.edu.au to request an account.\nWhen using these servers you should follow the best practices described in these documents.\nEach server has 10 - 30 TB of storage space which is not backed up. It is your own responsibility to have backups of your important data.\nMosquito Server (mozzie) IP address: 45.113.235.8\nThe mozzie server is a 28 core cluster comprised of two nodes. The master node has 11 cores available for use and 48 GB of RAM, while the worker node has 16 cores and 64 GB available.\nGenetic Rescue Server (rescue) IP address: 115.146.85.115\nThe rescue server is a 24 core cluster comprised of two machines. The master node has 11 cores available for use and 48 GB of RAM, while the worker node has 12 cores and 48 GB availble.\nSpartan Spartan is The University of Melbourne HPC system managed by Research Platform Services.\nTo get started with Spartan, you will need to request a Spartan account using your University of Melbourne email and apply for a new project or join an existing project.\nNectar cloud If you want to manage your own VM, you can apply for resources with the Nectar Research Cloud. Unless you\u0026rsquo;re comfortable being your own sysadmin, we recommend using the PEARG servers or Spartan before this option.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "  Compute Resources  An overview of high-performance computing resources available.\n Getting an Account  How to get an account on our clusters.\n Getting Help  How to get technical help.\n "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/metadata-documentation/using-mediaflux/",
	"title": "Using Mediaflux",
	"tags": [],
	"description": "How to upload your files into Mediaflux and set metadata.",
	"content": "The documents below cover the basics of using Mediaflux to create assets, set metadata and upload your data.\n1. Mediaflux Introduction\n How to log into Mediaflux Desktop What assets are stored in data/, individuals/, and libraries/ Metadata associated with assets  2. Adding Individual Assets\n Creating assets in individuals/ Setting sample metadata Modifying and deleting existing assets Using metadata templates  3. Adding Libraries to Assets\n Creating assets in libraries/ Setting library metadata  4. Uploading Libraries to Mediaflux\n Creating an archive containing your sequencing library data Uploading to Mediaflux using Mediaflux Desktop to data/  "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/usage-documentation/installing-software/",
	"title": "Installing Software",
	"tags": [],
	"description": "How to install software on our servers.",
	"content": " If you want a particular piece of software installed on mozzie or rescue you can email me at jchung@unimelb.edu.au. Anyone is also free to compile and install software in their personal directories.\nInstalling software on Linux can be challenging. Sometimes it\u0026rsquo;s as easy as a downloading a pre-compiled binary file and sometimes you\u0026rsquo;re stuck in Dependency Hell. There are also be many different ways to install software depending on which form it comes in. The most common ways you should use listed below.\nDownloading binaries or jar archives If the software is pre-compiled or is a java archive (ending in *.jar), you can just download it to your directory and run it.\nFor example, let\u0026rsquo;s say you want to install the latest version of Bowtie2 because the bowtie already installed on the cluster doesn\u0026rsquo;t have the new feature you want.\nYou would usually search for the software in your favourite search engine, then make your way to the download page. Bowtie2 is hosted on Sourceforge and has pre-compiled binaries, so we\u0026rsquo;ll download the software from there.\nThe Bowtie2 2.3.3 directory on Sourceforge currently has three items:\n bowtie2-2.3.3-macos-x86_64.zip bowtie2-2.3.3-linux-x86_64.zip bowtie2-2.3.3-source.zip  The first two items contain binaries for MacOS and Linux respectively. The \u0026ldquo;x86_64\u0026rdquo;refers to the architecture it\u0026rsquo;s meant to run on. The last file is an archive containing the source code if we want to compile it ourselves.\nSince our cluster runs Linux, we can download bowtie2-2.3.3-linux-x86_64.zip. Right click on the link and copy the link address so we can download it to the server.\n# Login and change to a directory where you want to store your software cd software # Download the file wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.3/bowtie2-2.3.3-linux-x86_64.zip/download -O bowtie2-2.3.3-linux-x86_64.zip # Unzip the directory unzip bowtie2-2.3.3-linux-x86_64.zip # Enter the directory and see what's inside cd bowtie2-2.3.3 \u0026amp;\u0026amp; ls # Run and view the help options of bowtie ./bowtie2 --help  Java archives are also simple to run. Just download the .jar file, and execute something like:\njava -jar \u0026lt;my_program\u0026gt;.jar  Sometimes you\u0026rsquo;ll need to specifiy the memory required like so:\n# Specify 4gb as the maximum heap size java -Xmx4g -jar \u0026lt;my_program\u0026gt;.jar  Conda packages Conda is a package manager written in Python and is available for use on our clusters. Package managers make installing easy by automating the process and handing dependencies. Many bioinformatics tools have been wrapped for Conda installation and are in the bioconda channel.\nYou\u0026rsquo;ll need to create your own virtual environment to use Conda, since you won\u0026rsquo;t have write permission to install software in the default location. A virtual environment is an environment where you have your own isolated copy of Conda and your own software packages that you installed via Conda. Virtual environments work by appending the virtual environment\u0026rsquo;s bin directory to your $PATH.\nYou can learn more about Conda here.\nIf you\u0026rsquo;re using the Melbourne Bioinformatics clusters, you\u0026rsquo;ll need to module load Python into your environment path first.\n# If you're using barcoo module load python-intel/3.6  Let\u0026rsquo;s create a new Conda virtual environment and install a package.\n# Create a virtual environment called my_new_env conda create -n my_new_env # Activate the enviroment source activate my_new_env # Your command line prompt should now start with (my_new_env) $ # Look at your path and see the first bin directory is your # conda environment in your home directory echo $PATH  You can manage packages using the conda command.\n# Print conda help conda -h # List conda packages conda list  You can install packages with conda install. For example, bioawk is available in the bioconda channel and you can install it like so:\n# Install bioawk from the bioconda channel conda install -c bioconda bioawk  When you\u0026rsquo;re finished using your software in conda, you can exit the virtual environment with:\nsource deactivate  And when you need your virtual environment again, you can reactivate it with:\nsource activate \u0026lt;my-environment-name\u0026gt;  Compiling from source Compiling from source is another way to install software. Sometimes, only the source code is provided and in this case, you\u0026rsquo;ll need to compile the software yourself.\nA common case is that the source code of the program you want to install is available on GitHub. Often, there are installation instructions in the README or INSTALL file which you can follow.\nIn the most simple case, a make command will run a Makefile and compile the software into a binary file which you can execute. Let\u0026rsquo;s compile bwa as an example.\n# Clone the repository git clone https://github.com/lh3/bwa.git # Change directory cd bwa # Compile make  A new file called bwa should be in the directory which you can run with ./bwa.\nSoftware may also specify it needs to be built with ./configure, make, then make install. The configure script checks the environment the software is to be built in and checks dependencies. Typically, you can also specify additional options when running ./configure such as --prefix=/home/my_username/bin to specify where to install the software. Running the configure script should output a Makefile. Running make will built the software and make install will copy the executable files into the default or specified directory.\nYou can read more about using configure, make, and make install here and here.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/best-practices/naming-files/",
	"title": "Naming Files",
	"tags": [],
	"description": "Best practices for naming files.",
	"content": " This document describes best practices for naming files.\nI highly recommend you read this Data Carpentary document on file naming.\nBe descriptive Good filenames should be precise. For example, a raw FASTQ filename received from AGRF may look something like this:\nX54321-1_AB1XXXXXX_GAATTCGT-ATAGAGGC_L007_R2.fastq.gz  The filename contains a lot of information which is descriptive to the user and can be easily extracted programatically.\nJust by looking at the filename and knowing what each field represents, we have the following information:\n Sample name: X54321-1 Flowcell ID: AB1XXXXXX Index: GAATTCGT-ATAGAGGC Lane: L007 Forward/Reverse: R2 File format: fastq Compression: gz  Storing metadata in this fashion allows us to avoid mixing up samples and losing expensive sequencing data due to lost information.\nBe consistent If you\u0026rsquo;re storing metadata in filenames, it\u0026rsquo;s good practice to be consistent with what each field represents. Being consisent also means filenames are sorted in a logical manner when listed alphabetically.\nGood consistency:\n2017-02-07_cg_meeting_notes_with_alice.md 2017-02-14_cg_meeting_notes_with_bob.md 2017-02-21_cg_meeting_notes_with_alice.md 2017-02-28_cg_meeting_notes_with_carol.md  Bad consistency:\n14.02.17_bob_cg_meeting.md 21-02-alice-meeting-cg.md carol_cg_meeting_notes_2017-02-28.md cg_meeting_with_alice_2017-02-07.md  Allowed characters Avoid special characters. The only characters you should be using in your filenames are the alphanumeric characters (A-Z, a-z, 0-9), underscores (_), periods (.), and dashes (-). Always start your filename with an alphanumeric character.\nAvoid using spaces Using spaces in filenames makes things difficult when working with UNIX. Therefore, use underscores _ and dashes - to separate words.\nFor example:\nsample-data-from-bob.txt 2017-03-03_raw_data_ID_A721BW.tar.gz  You can also use periods . for additional metadata information.\nsample-1.AaegL3.bam sample-1.AaegL3.sorted.bam sample-1.AaegL3.sorted.filtered.bam  YYYY-MM-DD The correct way to write numeric dates is YYYY-MM-DD, (i.e. ISO 8601).\nGood names:\n2017-08-01_results/ 2016-04-22_meeting_notes.txt 2018-11-29_raw-data-from-john-doe.tar.gz  Lowercase and uppercase Using purely lowecase letters, or a mixture of lower and uppercase letters in filenames are both ok. It\u0026rsquo;s a matter of personal preference.\nAvoid having files that differ only by case in the same directory. e.g.\nnotes.txt Notes.txt  "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/getting-started/getting-an-account/",
	"title": "Getting an Account",
	"tags": [],
	"description": "How to get an account on our clusters.",
	"content": "To obtain an account on mozzie or rescue, email jchung@unimelb.edu.au and request an account. Please include your preferred username in the email.\nAfter you SSH into the server, the first thing you should do is change your password. You can do this with the passwd command and follow the prompts.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/metadata-documentation/mediaflux-explorer/",
	"title": "Mediaflux Explorer",
	"tags": [],
	"description": "How to use Mediaflux Explorer.",
	"content": "The Mediaflux service can be accessed via a Java applet (Mediaflux Explorer) or via your web browser with Mediaflux Desktop. These clients allow you to view your data, transfer data, and edit metadata.\nThe Mediaflux Explorer Java applet should be used when transferring data and is available on both mozzie and rescue servers. Note that uploading large files via the browser is unreliable and slow, therefore Mediaflux Desktop should not be used for uploading.\nAssuming you have your data stored on one of our servers (mozzie or rescue) open your preferred internet browser and enter the IP address of the server into the navigation bar.\nYou should be taken to the dashboard page that lists the services available. Find the \u0026lsquo;Lubuntu Desktop\u0026rsquo; service and click on the access link to use VNC. This allows us to access the server using a graphical interface through a browser.\nEnter your credentials for your account to login. You may need to do this twice\u0026mdash; once to access VNC and again to login to the Lubuntu desktop.\nLogging in will bring you to the Lubuntu desktop where you should see icons including a \u0026lsquo;Mediaflux\u0026rsquo; icon. Double click it to launch Mediaflux Explorer.\nSelect the HTTPS protocol in the dropdown menu and enter mediaflux.vicnode.org.au as the host. The port should automatically change to 443.\nIn the \u0026lsquo;Domain\u0026rsquo; field enter aaf and then move to the next field. A dropdown menu should then appear below the \u0026lsquo;Domain\u0026rsquo; field (if it doesn\u0026rsquo;t show up immediately, wait a few seconds). Select The University of Melbourne from the listed institutions.\nFinally, enter your unimelb username (not your email) and password to sign in.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/usage-documentation/stacks-databases/",
	"title": "Stacks Databases",
	"tags": [],
	"description": "Best practises for working with Stacks databases.",
	"content": " Database management Databases are usually large in size and we have limited space on our clusters. To remedy this, please remove any databases you don\u0026rsquo;t need. If you have a database older than 3 months in the system, you will be asked to clean up your old databases.\nNaming your databases Your database name must end with _radtags to show up in the web interface.\nMake sure to include your name and the date your database was created in your database name. Some examples of good database names are:\nalice_mozzie_20170304_radtags bob_possum_filtered_20170701_radtags carol_AaegL2_males_20170122_radtags  If your name and a creation date is not included in your database name, your database may be deleted without warning.\n Removing databases Delete databases by using the SQL DROP DATABASE statement.\nmysql -e \u0026quot;DROP DATABASE jess_20170815_radtags\u0026quot;  Backing up databases You can also backup your database with the mysqldump command. The output is in plain-text, so use gzip to compress the file to save space.\nmysqldump --databases jess_20170815_radtags \\ | gzip \\ \u0026gt; ~/my_backups/jess_20170815_radtags.sql.gz  Restoring a database from backup You can restore your saved databases by importing your file into MySQL.\n# Uncompress your file gunzip jess_20170815_radtags.sql.gz # Import from file mysql jess_20170815_radtags \u0026lt; ~/my_backups/jess_20170815_radtags.sql  Running Stacks on clusters which don\u0026rsquo;t allow databases If you\u0026rsquo;re using the barcoo cluster for your analysis, you won\u0026rsquo;t be able to load your catalog to a database on the cluster. This means you\u0026rsquo;ll need to do your computation (on barcoo) separately to a machine that alows database access.\nOn barcoo If you\u0026rsquo;re using ref_map.pl or denovo_map.pl you can use the -S option to disable all database interaction. The output from this step should be many tsv files.\nExample output:\nbatch_1.catalog.alleles.tsv batch_1.catalog.snps.tsv batch_1.catalog.tags.tsv batch_1.haplotypes.tsv batch_1.hapstats.tsv batch_1.markers.tsv batch_1.populations.log batch_1.sumstats_summary.tsv batch_1.sumstats.tsv batch_1.vcf mozzie-1.alleles.tsv mozzie-1.matches.tsv mozzie-1.models.tsv mozzie-1.snps.tsv mozzie-1.tags.tsv mozzie-2.alleles.tsv mozzie-2.matches.tsv mozzie-2.models.tsv mozzie-2.snps.tsv mozzie-2.tags.tsv ...  Transfer your data You\u0026rsquo;ll need to transfer your output files from ref_map.pl or denovo_map.pl to another computer which you have permission to create databases on. Since tsv files are plain-text, it\u0026rsquo;s good practice to compress them before transferring.\nYou can use a graphical FTP client or the command line to transfer files. Here we\u0026rsquo;ll use rsync with the --compress option to do transfer the files.\nOn barcoo, in an interactive slurm session, you can transfer the directory called catalog with the following command:\nrsync -av --compress --update --progress \\ catalog \\ \u0026lt;username\u0026gt;@\u0026lt;IP-address\u0026gt;:\u0026lt;path_to_my_project_directory\u0026gt;  replacing \u0026lt;username\u0026gt;, \u0026lt;IP-address\u0026gt;, and \u0026lt;path_to_my_project_directory\u0026gt; with your relevant information.\nLoad the catalog with load_radtags.pl On your machine which you have permission to create databases, load your catalog with load_radtags.pl.\n# Create an empty database mysql -e \u0026quot;CREATE DATABASE jess_20170815_radtags\u0026quot; # Load template tables mysql jess_20170815_radtags \u0026lt; /mnt/galaxy/gvl/software/stacks/share/stacks/sql/stacks.sql # Load RAD tags load_radtags.pl \\ -D jess_20170815_radtags \\ -p catalog \\ -b 1 -B -e \u0026quot;Description here\u0026quot; \\ -c  Index the catalog with index_radtags.pl You\u0026rsquo;ll then need to create indices for your database. Use -c to generate the catalog index and -d to generate the unique tags index.\nindex_radtags.pl \\ -D jess_20170815_radtags \\ -c \\ -t  View your data in the web interface With your web browser, go to the server you have your data on, and navigate to the Stacks page. Click on your database name to view it.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/usage-documentation/rad-seq-pipeline/",
	"title": "Using the RAD-seq Pipeline",
	"tags": [],
	"description": "How to use radpipe: the Hoffmann Lab RAD-seq pipeline",
	"content": " If you\u0026rsquo;re using the mozzie or rescue server and want to use Stacks \u0026gt;= 2.0 to perform your RAD-seq data processing and analysis, then you have the option of using the RAD-seq pipeline: radpipe.\nTo use the pipeline, it is highly recommended you have some knowledge on how to use Screen or another terminal multiplexer such as Tmux. This is because the pipeline can run for hours or days and a terminal multiplexer will allow you to detach the session so it can continue running when you disconnect from the server. These instructions assume you are familiar with Screen.\nIf you want to run radpipe on another system such as your own virtual machine or local computer, you\u0026rsquo;ll need to install it and the dependencies described here.\nNote that radpipe does not support the Stacks de-novo assembly workflow (yet). To use the pipeline, you will need a reference genome to align your sequence data to.\n Project setup Before you begin any analysis you need to create your project directory to store your analysis.\nYour project directory should follow our best practices guidelines.\nFor example:\n# I like to store my projects in a directory named 'projects' located in my home directory cd ~/projects # Give your project a name that makes sense mkdir super_rad_project \u0026amp;\u0026amp; cd super_rad_project # Create directories for the data, results, and reference files mkdir data results reference  If you don\u0026rsquo;t have your reference genome for your organism on the server yet, download it to the reference/ directory. If the reference genome is already on the server, I recommend you move or or symlink the file to the reference directory.\nDownload your sequencing FASTQ files and place them in your data/ directory. Make sure your data/ directory has a separate directory for each sequencing run or library. Each directory must contain two files from Illumina sequencing, a FASTQ file containing forward reads (R1) and a FASTQ file containing reverse reads (R2).\nEach library should have a barcodes file containing the barcode pair and the sample name separated by tabs. For example:\nACGTCA\tCATGAC\tA110 GTACTG\tCATGAC\tE002 CTAGTC\tCATGAC\tX433 AGCTGA\tCATGAC\tA8814  If using the mozzie or rescue servers and symlinking data make sure your paths begin with /mnt/galaxy/home/my_username/... and not /home/my_username/.... This is because jobs running on worker nodes need the /mnt/ path to find the files as the user directory symlinks in /home/ don\u0026rsquo;t exist on the worker nodes.\n Pipeline stages The pipeline is composed of a number of stages.\n fastqc: Runs FastQC on raw sequencing files and outputs to results/qc/fastqc/. multiqc_fastqc: Runs MultiQC on FastQC outputs and outputs to results/qc/. process_radtags: Runs Stacks process_radtags to demux samples and outputs to results/sample_radtags/. build_index: Builds index for the reference FASTA file for either bwa or bowtie and stores indices in results/ref/. alignment: Aligns FASTQ files with either bwa or bowtie and outputs BAMs to results/alignments/. sort_bam: Sorts BAM files by coordinate and generates indices. Outputs to results/alignments/. filter_bam: Optional step to filter BAM files using Samtools view. Outputs to results/alignments/. flagstat: Runs Samtools flagstat on the final BAM files and outputs to results/qc/flagstat/. multiqc_flagstat: Runs MultiQC on the flagstat outputs and outputs to results/qc/. gstacks: Runs Stacks gstacks on sorted BAMs and outputs to results/gstacks/. populations: Runs Stacks populations and outputs to results/populations/.  Ruffus (the framework the pipeline uses) works by requiring you to specify a target task. The pipeline will then work out which tasks the target task depends on, and run them automatically.\nCreate a pipeline config file radpipe reads a configuration file to define how the pipeline stages will be run. This config file is written in YAML.\nCopy the below block, and paste the contents in a file named pipeline.config in your project directory.\n#--------------------------------- # CONFIG #--------------------------------- pipeline_id: radpipe # The directory the pipeline will write files. results_dir: results/ # The reference genome in FASTA format. reference_genome: ref/mppdraft3.fa # Sequencing data # Each sequencing run / library should be in its own directory. In each # directory, there should be two Illumina sequncing read files in *fastq.gz # format (R1 and R2) and a text file containing barcodes in the format that # process_radtags accepts. FASTQ filenames should be unique. # Note that the directory name will be used as the read group ID for when # performing alignment. libraries: lib_01: lib_dir: data/lib_01 r1: BparvusLib1_S1_R1_001.fastq.gz r2: BparvusLib1_S1_R2_001.fastq.gz barcodes: lib1_barcode.txt lib_02: lib_dir: data/lib_02 r1: BparvusLib2_S2_R1_001.fastq.gz r2: BparvusLib2_S2_R2_001.fastq.gz barcodes: lib2_barcode.txt #--------------------------------- # PROCESS RADTAG OPTIONS #--------------------------------- # The two restriction enzymes used. renz_1: sphI renz_2: mluCI # Additional process_radtag options # http://catchenlab.life.illinois.edu/stacks/comp/process_radtags.php # Add PCR adapter options if necessary: # --adapter_1 AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAGACCGATCTCGTATGCCGTCTTCTGCTTG # --adapter_2 AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT process_radtags_options: \u0026quot;-t 60 -c -q -r -s 10\u0026quot; #--------------------------------- # ALIGNMENT OPTIONS #--------------------------------- # Which program to use for alignment. Choose from [bowtie, bwa mem] alignment_method: bwa mem # Additional alignment options # bwa mem: http://bio-bwa.sourceforge.net/bwa.shtml alignment_options: \u0026quot;-a\u0026quot; # bowtie: http://bowtie-bio.sourceforge.net/manual.shtml # alignment_options: \u0026quot;-v 3 --tryhard --best\u0026quot; #--------------------------------- # BAM FILTERING OPTIONS #--------------------------------- # Samtools view filtering options. Comment out to skip bam filtering step. # samtools_view_options: \u0026quot;-f 3 -q 30\u0026quot; #--------------------------------- # STACKS GSTACKS OPTIONS #--------------------------------- # Extra options for gstacks # http://catchenlab.life.illinois.edu/stacks/comp/gstacks.php gstacks_options: \u0026quot;\u0026quot; #--------------------------------- # STACKS POPULATIONS OPTIONS #--------------------------------- # Analysis ID # If you plan to run multiple populations runs with different popmap files, give # each run a different name, which will create new directories in the results # directory instead of overwriting them analysis_id: analysis_v1 # Custom popmap file (optional) # If this is left blank, the pipeline will create a popmap file with all samples # in the same population group. Note that this is for stacks populations # only (gstacks will use all available samples) # popmap_file: test_popmap.txt # Values of r for which Stacks populations will run. A new directory will # be created for each run populations_r: - 0.5 - 0.75 - 0.8 # Extra options for populations. Note that the --vcf option will be # automatically included when running the pipeline. # http://catchenlab.life.illinois.edu/stacks/comp/populations.php populations_options: \u0026quot;--min_maf 0.05 --write_random_snp --hwe --fstats --genepop\u0026quot; #--------------------------------- # SLURM PIPELINE CONFIG #--------------------------------- # Default settings for the pipeline stages. # These can be overridden in the stage settings below. defaults: # Number of CPU cores to use for the task cores: 1 # Maximum memory in gigabytes for a cluster job mem: 4 # VLSCI account for quota account: VR0002 queue: main # Maximum allowed running time on the cluster in Hours:Minutes walltime: '1:00' # Load modules for running a command on the cluster. modules: # Run on the local machine (where the pipeline is run) # instead of on the cluster. False means run on the cluster. local: False # Stage-specific settings. These override the defaults above. stages: fastqc: walltime: '2:00' multiqc: walltime: '1:00' process_radtags: walltime: '8:00' mem: 4 build_index: walltime: '8:00' mem: 16 alignment: walltime: '8:00' cores: 2 mem: 8 sort_bam: walltime: '4:00' mem: 4 filter_bam: walltime: '1:00' flagstat: walltime: '1:00' gstacks: walltime: '24:00' cores: 8 mem: 32 populations: walltime: '24:00' cores: 4 mem: 8  Carefully read through the config file and edit the options to fit with your analysis.\n Edit the libraries dictionary to contain your library directories and filenames. Edit the Stacks process radtags options if necessary. Edit the alignment options if necessary. Note that the pipeline supports both BWA mem and Bowtie using the alignment_method option. Edit the BAM filtering options if necessary. If samtools_view_options is commented out, filtering will be skipped. Edit the Stacks populations options if necessary. If you have a popmap file, uncomment the popmap_file option and provide the path to your file. The remaining SLURM pipeline options should be left as is unless your jobs are running out of walltime or memory.  Executing the pipeline If you\u0026rsquo;re using the mozzie or rescue servers, radpipe is already installed.\nBefore you begin, start up a screen session. For example:\n# Create and attach a screen session named 'pipeline' screen -S pipeline  radpipe is installed inside a Python virtual environment. You can activate the virtual environment with:\nsource /mnt/galaxy/gvl/software/radpipe/bin/activate  What is a virtual environment?\nOn the server, radpipe is installed inside a Python virtual environment.\nTODO: briefly explain virtual environments\n First, see if you can run radpipe and look at the usage information the program has with the -h option.\nradpipe -h  The pipeline runs by specifying a target task. All stages that are required for that task to run are executed. For example, running radpipe with --target_task=alignment, the pipeline will run the process_radtags, build_index, and alignment steps. If you want to know which tasks will be run when you specify a particular target_task, you can refer to the above flowchart, then look for your task and follow all the dependencies upward to see what will be run.\nTry using the -n (or --just_print) option to perform a dry run of the pipeline.\nradpipe \\ --config pipeline.config \\ --verbose \\ --target_tasks alignment \\ -n  Something like the following should be printed on to the screen:\n________________________________________ Tasks which will be run: Task = 'radpipe::reference_genome' Task = 'radpipe::process_radtags' Task = 'radpipe::bwa_index' Task = 'radpipe::bwa_mem' ________________________________________  Note that since \u0026ldquo;bwa mem\u0026rdquo; is specified as the alignment method in the configuration file, the build_index task is named bwa_index and the alignment task is named bwa_mem.\nI recommend always running radpipe with the -n option to print what tasks will be run before running jobs for real.\nVerbosity level:\nThe --verbose option allows you to set the verbosity (how much information the program prints out to the screen) of the pipeline. It\u0026rsquo;s useful to increase the verbosity level if you\u0026rsquo;re troubleshooting a problem. Generally, I recommend --verbose 3 as a good level of verbosity for radpipe.\n You can also generate a flowchart image with the --flowchart option.\nradpipe \\ --config pipeline.config \\ --target_tasks populations,multiqc_fastqc,multiqc_flagstat \\ --flowchart_format png \\ --flowchart radpipe.png  Running the pipeline It is recommended to run fastqc and multiqc_fastqc before you begin processing your data. After MultiQC finishes, view the multiqc_report.html file by transferring the html file to your computer and opening it in your web browser. Alternatively you could also move the html file to the your public_html directory and view the file remotely.\nMake sure you\u0026rsquo;re inside a screen session before running the pipeline. You can run the pipeline to multiqc_fastqc with the following:\nradpipe \\ --config pipeline.config \\ --verbose 3 \\ --log_file radpipe.log \\ --target_tasks multiqc_fastqc \\ --jobs 8 \\ --use_threads  The --jobs option allows the pipeline to submit multiple jobs to the SLURM queue. This will result in a maximum of 8 jobs in your queue. If there are more than 8 jobs in total, the jobs will be submitted one at a time as jobs get completed.\nHow many jobs should I run at a time?\nIf you\u0026rsquo;re the only person running jobs on the server, you can use as many CPUs as there are available. SLURM will handle the resources automatically, so jobs will wait in the queue until there are enough resouces available for that particular job to run. However, if you\u0026rsquo;re sharing the server with other people, please limit the number of jobs you\u0026rsquo;re submittiing as the SLURM scheduling configuration is set to run jobs as First In, First Out.\n Once your jobs are running, in another terminal session or screen session, try checking the SLURM queue (and if you need to refresh your memory on SLURM commands, you can do so here).\nsqueue  You should see a list of your jobs running and which node they\u0026rsquo;re running on. e.g.\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1843 main radpipe_ jess R 3:30 1 master 1844 main radpipe_ jess R 2:39 1 w1  The job names in the queue are too long, so they appear cut off. You can use squeue\u0026rsquo;s -o option to specify the output format and make the job name column wider.\nsqueue -o \u0026quot;%.10i %.22j %.8u %.8T %.10M %.9l %.6D %R\u0026quot;  Using the --log_file option will create a log file called radpipe.log which will keep track of what commands you ran.\nAs an addition layer of logging, Slurm job scripts are written in the newly created jobscripts/ directory. If a job fails, and you\u0026rsquo;re trying to troubleshoot why, you can check the stderr files in this directory.\nYou can also increase the verbosity by supplying a number after the --verbose option, e.g. --verbose 3.\nIf an individual task of the pipeline fails (e.g. due to running out of memory), the pipeline will stop running after the jobs in the SLURM queue are finished. When you run radpipe again, the pipeline will only run the jobs that need running (i.e. it won\u0026rsquo;t re-run the previously completed steps). However, if you update an earlier stage in the pipeline (e.g. changing the reference genome file), all the downstream tasks will be indentified as being out-of-date, and the pipeline will attempt to run these tasks again the next time you specify a target task that depends on the out-of-date task.\nHow to kill a running pipeline If you want to kill a running pipeline that is running on the cluster and ^C isn\u0026rsquo;t killing the pipeline process, you will need to cancel all individual pipeline jobs in the system. With SLURM, you can cancel your jobs with scancel \u0026lt;job_id\u0026gt;. If all your jobs are from radpipe, you can cancel all jobs submitted with scancel --user=\u0026lt;your_username\u0026gt;.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/metadata-documentation/sample-map/",
	"title": "Sample Map",
	"tags": [],
	"description": "Interactive map of sample data in Mediaflux.",
	"content": "Enter the password to view the interactive map of the sample data in the Hoffmann Lab Mediaflux project.\nEnter    \"use strict\"; function loadPage(pwd) { var hash = pwd; hash = Sha1.hash(pwd); var url = \"http://45.113.232.220/secret/\" + hash + \"/sample_map.html\"; window.open(url, '_blank'); } $(\"#loginbutton\").on(\"click\", function() { loadPage($(\"#password\").val()); }); $(\"#password\").keypress(function(e) { if (e.which == 13) { loadPage($(\"#password\").val()); } }); $(\"#password\").focus(); $(document).ready(function() { $(\"#loginbutton\").hover(function() { $(this).css(\"background-color\", \"#2E53A5\"); }, function() { $(this).css(\"background-color\", \"#008CBA\"); }); });  Last updated 2018-09-07.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/best-practices/backing-up-your-data/",
	"title": "Backing Up Your Data",
	"tags": [],
	"description": "Best practices for backing up your data.",
	"content": " If you only have a single copy of your data, you\u0026rsquo;re vulnerable to losing it. A hardware failure or theft can lead to losing hundreds of hours of work or irreplacable photos.\n\u0026ldquo;Don\u0026rsquo;t worry\u0026mdash; I have a Time Machine backup,\u0026rdquo; I hear you say. Having a Time Machine backup is a good first step, however, you\u0026rsquo;re still at risk in cases where you lose both copies in a house fire or if both your laptop and Time Machine hard drive are stolen.\nHard disk drives are also prone to failure.\n\nOne solution is to store a copy of your data remotely in the cloud. Storage services will also have redundancy measures in place to avoid data loss when hard drives inevitably fail.\nA good rule to follow is the 3-2-1 rule for backing up your data.\nThe 3-2-1 rule for backups The 3-2-1 rule for backup goes as follows:\n Have at least three copies of your data Use at least two different types of media for your copies At least one of your copies should be offsite    Backing up your experimental data Following the 3-2-1 rule, your sequencing data for your study should have at least three copies. A typical case would be:\n one copy on a external hard drive stored in the lab one copy on the volume attached to the server where you\u0026rsquo;re doing your analysis one copy in Mediaflux labeled with appropriate metadata  Cloud-based automated backup services Having a remote backup for your computer is also a good idea. Services such as Backblaze can automatically backup your computer, but they also have a monthly subscription fee at around $5 a month.\nAn example of using the 3-2-1 rule for backing up your laptop would be:\n one copy on the SSD in your laptop one copy on a HDD with Time Machine backups one copy in the cloud using a cloud backup service such as Backblaze  "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/getting-started/getting-help/",
	"title": "Getting Help",
	"tags": [],
	"description": "How to get technical help.",
	"content": "For help related to getting/using compute resources or queries related to bioinformatics, email Jessica Chung (jchung@unimelb.edu.au).\nFor help related to Mediaflux and metadata, email Tom Schmidt (tom.schmidt@unimelb.edu.au) or Jessica Chung (jchung@unimelb.edu.au).\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/best-practices/",
	"title": "Best Practices",
	"tags": [],
	"description": "",
	"content": "  Directory Structure  How to structure your project directory when performing an analysis.\n Naming Files  Best practices for naming files.\n Backing Up Your Data  Best practices for backing up your data.\n "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/metadata-documentation/",
	"title": "Metadata Documentation",
	"tags": [],
	"description": "",
	"content": "  Metadata Overview  A overview of our metadata system in use.\n Using Mediaflux  How to upload your files into Mediaflux and set metadata.\n Mediaflux Explorer  How to use Mediaflux Explorer.\n Sample Map  Interactive map of sample data in Mediaflux.\n "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/usage-documentation/",
	"title": "Usage Documentation",
	"tags": [],
	"description": "",
	"content": "  Running Jobs with SLURM  Documentation on how to run SLURM jobs.\n Installing Software  How to install software on our servers.\n Stacks Databases  Best practises for working with Stacks databases.\n Using the RAD-seq Pipeline  How to use radpipe: the Hoffmann Lab RAD-seq pipeline\n "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/tutorials/",
	"title": "Tutorials",
	"tags": [],
	"description": "",
	"content": "  Introduction to RAD-Seq  Introduction to Reference-based ddRAD-Seq using Stacks.\n "
},
{
	"uri": "https://pearg.github.io/pearg_documentation/_header/",
	"title": "header",
	"tags": [],
	"description": "",
	"content": "PEARG Documentation\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://pearg.github.io/pearg_documentation/",
	"title": "PEARG Documentation",
	"tags": [],
	"description": "Bioinformatics documentation for PEARG lab members",
	"content": " PEARG Documentation This site contains bioinformatics-related documentation for PEARG lab members.\nGetting started These documents go over what compute resources are available to you, how to get an account on our clusters, and some terms of conditions.\n HPC resources overview Getting an account  Best practices These documents describe some best practices when using our compute resources. Not following some of these guidelines may result in a scolding.\n Directory structure Naming files Backing up your data  Metadata Documentation These documents describe how to use our metadata system to record information relating to your samples.\n Metadata overview Using Mediaflux  Usage Documentation Documentation related to using our clusters.\n Running jobs with SLURM  Installing software Stacks databases Reference data  Tutorials Tutorials to help you get started in bioinformatics.\n Introduction to RAD-seq using Stacks  Contribute to this documentation This documentation\u0026rsquo;s source code is hosted on GitHub. Feel free to update the content\u0026mdash; just click the Edit this page link displayed on top right of each page, and create a pull request when done.\nContact Contact me at jchung@unimelb.edu.au or submit an issue on GitHub.\n"
},
{
	"uri": "https://pearg.github.io/pearg_documentation/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]